{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Topic analysis based on Genius songs lyrics\n","\n","Author : lievre.thomas@gmail.com\n","\n","---\n","\n","In this notebook, we will explore the genius data extract from [Kaggle](https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information).\n","\n","**The aim of this analysis is to retrieve topic from lyrics and retrieve main topics by year or decade.**\n","\n","This notebook was carried out in the context of a class project imposed by the [text mining course (TDDE16)](https://www.ida.liu.se/~TDDE16/project.en.shtml) of Linköpings universitet.\n","\n","\n","## Few informations about Genius website\n","\n","Genius is an American digital company founded on August 27, 2009, by Tom Lehman, lan Zechory, and Mahbod Moghadam. Originally launched as Rap Genius with a focus on hip-hop music, it was initially a crowdsourced website where people could fill in the lyrics of rap music and give an interpretation of the lyrics. Over the years the site has grown to contain several million annotated texts from all eras ( from [Wikipedia Genius page](https://en.wikipedia.org/wiki/Genius_(company))).\n","\n","\n","## Load the data in memory\n","\n","Data are all contain in a big 9GB csv file (around 5 millions rows). It could be difficult to load all this data in our computer memory. To deal with this issue, I made a loading class to split the data in 6 pickles files to improve the compressness of the data which aim to improve the loading speed in the memory. Then the pickles are randomly draw to improve generality of the data. We currently assumed the data pickles batch are identically distributed (we will explore the data batches at the second part). The class below deal with all the process."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:00:44.199453Z","iopub.status.busy":"2023-05-22T11:00:44.199018Z","iopub.status.idle":"2023-05-22T11:00:44.240434Z","shell.execute_reply":"2023-05-22T11:00:44.23943Z","shell.execute_reply.started":"2023-05-22T11:00:44.199407Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from random import seed, sample\n","import pickle\n","import glob\n","import os\n","\n","class Loader():\n","\n","    def __init__(self, in_path, out_path):\n","        \"\"\"\n","        Args:\n","            in_path (str): csv input path.\n","            out_path (str): Output directory path to store the pickles.\n","            chunksize (int, optional): Chunksize for DataFrame reader. Defaults to 10**6. \n","        \"\"\"\n","\n","        self.__in_path = in_path\n","        self.__out_path = out_path\n","        self.__chunksize = 10**6\n","\n","    def __produce_pickles(self):\n","        \"\"\"produce pickles by reading csv by chunksize\n","        \"\"\"\n","        with pd.read_csv(self.__in_path, chunksize = self.__chunksize) as reader:\n","            try:\n","                os.makedirs(self.__out_path)\n","            except FileExistsError:\n","                # directory already exists\n","                pass\n","            for i, chunk in enumerate(reader):\n","                out_file = self.__out_path + \"/data_{}.pkl\".format(i+1)\n","                with open(out_file, \"wb\") as f:\n","                    pickle.dump(chunk, f, pickle.HIGHEST_PROTOCOL)\n","    \n","    def load_pickle(self, pickle_id):\n","        \"\"\"load a pickle file by id\n","\n","        Args:\n","            pickle_id (int): pickle id.\n","\n","        Raises:\n","            Exception: The path of the given id isn't a file\n","\n","        Returns:\n","            obj: DataFrame\n","        \"\"\"\n","        # produce the pickles if the directory not exists or\n","        # if the directory is empty \n","        if (not os.path.exists(self.__out_path)) or \\\n","              (len(os.listdir(self.__out_path)) == 0):\n","            self.__produce_pickles()\n","        \n","        # get the file path following the pickle_id\n","        # given in parameter\n","        file_path = self.__out_path + \\\n","            \"/data_\" + str(pickle_id) + \".pkl\"\n","\n","        if os.path.isfile(file_path):\n","            df = pd.read_pickle(file_path)\n","        else:\n","            raise Exception(\"The pickle file data_{}.pkl doesn't exist\".format(pickle_id))\n","        return df\n","        \n","\n","    def random_pickles(self, n_pickles = 3, init = 42, verbose = True):\n","        \"\"\"random reader over pickles files\n","\n","        Args:\n","            n_pickles (int, optional): number of pickles to load. Defaults to 3.\n","            init (int, optional): Integer given to the random seed. Defaults to 42.\n","            verbose (bool, optional): Print the loaded files. Defaults to True\n","\n","        Raises:\n","            Exception: Stop the process if n_pickles exceed pickle files number.\n","\n","        Returns:\n","            obj: pd.Dataframe\n","        \"\"\"\n","\n","        # produce the pickles if the directory not exists or\n","        # if the directory is empty \n","        if (not os.path.exists(self.__out_path)) or \\\n","              (len(os.listdir(self.__out_path)) == 0):\n","            self.__produce_pickles()\n","\n","        pickle_files = [name for name in\n","                        glob.glob(self.__out_path + \"/data_*.pkl\")]\n","        # draw p_files        \n","        seed(init)\n","\n","        if n_pickles <= 6:\n","            random_p_files = sample(pickle_files, n_pickles)\n","        else:\n","            raise Exception(\"The parameter n_pickles (\" +\n","                            \"{}) exceed the numbers of pickle files ({})\"\\\n","                                .format(n_pickles, len(pickle_files)))\n","        # print the drawed files\n","        if verbose:\n","            print(\"Loaded pickles:\")\n","            for p in random_p_files:\n","                print(p)\n","\n","        # load random pickles file\n","        df_list = [pd.read_pickle(p) for p in random_p_files]\n","\n","        # create the dataframe by concatenate the previous\n","        # dataframes list\n","        df = pd.concat(df_list, ignore_index = True)\n","        return df"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:00:51.232703Z","iopub.status.busy":"2023-05-22T11:00:51.231786Z","iopub.status.idle":"2023-05-22T11:00:51.237826Z","shell.execute_reply":"2023-05-22T11:00:51.236254Z","shell.execute_reply.started":"2023-05-22T11:00:51.23266Z"},"trusted":true},"outputs":[],"source":["# create reader\n","#  /!\\ change path in kaggle\n","kaggle_input = \"song_lyrics.csv\"\n","kaggle_output = \"./kaggle/working/data/\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:00:53.368594Z","iopub.status.busy":"2023-05-22T11:00:53.368182Z","iopub.status.idle":"2023-05-22T11:05:54.577342Z","shell.execute_reply":"2023-05-22T11:05:54.575632Z","shell.execute_reply.started":"2023-05-22T11:00:53.368555Z"},"trusted":true},"outputs":[],"source":["# initiate the file loader\n","loader = Loader(in_path = kaggle_input, out_path = kaggle_output)\n","\n","# load pickle 3\n","df = loader.load_pickle(3)"]},{"cell_type":"markdown","metadata":{},"source":["Batchs of data are randomly loaded in the memory. The number of batchs loaded depends on the memory capacity of the computer running the script. For the analysis, we will only works on the random samples loaded (All the data in Kaggle).  \n","\n","# Exploring the coarse data\n","\n","Let's visualize and explore the coarse data before a part of deeper analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["For each songs, we've got several informations :\n","- title of the song\n","- the tag (which kind of music)\n","- the artist singer name\n","- the release year\n","- the number of page views\n","- the featuring artists names\n","- the lyrics\n","- the genius identifier\n","- Lyrics language according to [CLD3](https://github.com/google/cld3). Not reliable results are NaN. CLD3 is a neural network model for language indentification.\n","- Lyrics language according to [FastText's langid](https://fasttext.cc/docs/en/language-identification.html). Values with low confidence (<0.5) are NaN. FastText's langid is library developped by Facebook’s AI Research lab for efficient learning of word representations and sentence classification. fastText has also published a fast and accurate tool for text-based language identification capable of recognizing more than 170 languages.\n","- Combines language_cld3 and language_ft. Only has a non NaN entry if they both \"agree\".\n","\n","More information at this link : https://www.kaggle.com/datasets/carlosgdcj/genius-song-lyrics-with-language-information"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# display the size\n","print('Data frame size (row x columns):', df.size)\n","print('Data rows number: ', len(df))\n","print('Number of unique songs (following genius id): ', len(df.id.unique()))"]},{"cell_type":"markdown","metadata":{},"source":["Genius id seems to be the unique rows identifier.\n","\n","Let's vizualise size of the coarse data over years before preprocessing to compare batch distributions. One things to know before vizualise the data, the pickles are create by chunks reading. "]},{"cell_type":"markdown","metadata":{},"source":["The last diplayed table gives us some information about the data. The csv file seems to be sort by id, so the pickle files are then sort too."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:54.585248Z","iopub.status.busy":"2023-05-22T11:05:54.583899Z","iopub.status.idle":"2023-05-22T11:05:57.674807Z","shell.execute_reply":"2023-05-22T11:05:57.673559Z","shell.execute_reply.started":"2023-05-22T11:05:54.585181Z"},"trusted":true},"outputs":[],"source":["import plotly.express as px\n","import plotly.io as pio\n","\n","# change theme template for every graph below\n","pio.templates.default = \"plotly_white\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get some information about the pickle data\n","def pickle_informations(data = loader):\n","    rows = []\n","    for i in range(1, len(os.listdir('data')) + 1):\n","        df = data.load_pickle(i)\n","        rows.append(len(df))\n","        del df\n","    return rows\n","\n","# get the rows\n","rows = pickle_informations()\n","\n","# create the dataframe\n","df_data = pd.DataFrame(\n","    {'batch' : ['data ' + str(i) for i in range(1,len(rows) + 1)],\n","    'rows' : rows})\n","\n","fig = px.bar(df_data, x=\"batch\", y=\"rows\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["Batch seems to have the same number of rows rexcept for the last one which is consistent because batch are create iteratively by 10e6 chunks over the csv The last batch could be seen as a rest."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:57.676521Z","iopub.status.busy":"2023-05-22T11:05:57.67619Z","iopub.status.idle":"2023-05-22T11:05:57.684243Z","shell.execute_reply":"2023-05-22T11:05:57.680395Z","shell.execute_reply.started":"2023-05-22T11:05:57.676488Z"},"trusted":true},"outputs":[],"source":["import plotly.graph_objects as go"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def add_bar(i, y1, y2, color, data = loader):\n","    df = data.load_pickle(i)\n","    df = df[(df.year >= y1) & (df.year <= y2)]\n","    df_year = df.groupby(['year']).size().reset_index(name='count')\n","    new_bar = go.Bar(\n","                x = df_year.year.values,\n","                y = df_year['count'].values,\n","                name = 'data_'+ str(i),\n","                marker = {'color' : color})\n","    new_trend = go.Scatter(\n","                x = df_year.year.values,\n","                y = df_year['count'].values,\n","                mode=\"lines\",\n","                line={'color' : color,\n","                    'width' : 0.5},\n","                showlegend=False)\n","    del df_year, df\n","    return new_bar, new_trend\n","\n","\n","def multi_barplot(year1, year2, colors):    \n","    # create a empty plotly.Figure object\n","    fig = go.Figure() \n","    # compute the batch number\n","    n_batch = len(os.listdir('data'))\n","    # test the color list feed in argument\n","    # fit well with the batch number\n","    if n_batch > len(colors):\n","        raise Exception(\n","            \"The colors list size({})doesn't \".format(len(colors)) +\n","            \"fit with the number of data\".format(n_batch))\n","    for i in range(1, n_batch + 1):\n","        fig.add_traces((add_bar(i, year1, year2, colors[i-1])))\n","    fig.update_layout(\n","        title = \"Data distribution over years ({} - {})\"\n","            .format(year1, year2),\n","        xaxis_title=\"years\",\n","        yaxis_title=\"title\",\n","        legend_title=\"Data batch\")\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import plotly.colors as col\n","\n","# create the color list\n","colors = col.qualitative.Plotly\n","\n","# 1990 - 2023\n","fig1 = multi_barplot(1960, 1989, colors)\n","fig1.show()\n","# 1960 - 1990\n","fig2 = multi_barplot(1990, 2023, colors)\n","fig2.show()"]},{"cell_type":"markdown","metadata":{},"source":["The first bar chart (1960 - 1989) shows an increasing numbers of data over years. Moreover batch seems to have quite similar distriutions over years. data_1 and data_2 batch quite outperform the 4 others. data_6 batch is weaker than the other due to its poor number of rows.\n","The data behaves similarly until 2012 as we can see on the second chart (1990-2023). After this year there is great increasing of the data retrieved. A minimum increase of at least 100% of the batch can be observed. An increase of up to 50 times the batch size for some."]},{"cell_type":"markdown","metadata":{},"source":["# Data pre-processing\n","\n","The aim of this part is to preprocess data in order to get suitable data for the analysis. let's focus on the year variable.\n","\n","We will focus on English songs, to facilitate the analysis and the work of natural language processing algorithms."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:57.687737Z","iopub.status.busy":"2023-05-22T11:05:57.687289Z","iopub.status.idle":"2023-05-22T11:05:58.032745Z","shell.execute_reply":"2023-05-22T11:05:58.031326Z","shell.execute_reply.started":"2023-05-22T11:05:57.687666Z"},"trusted":true},"outputs":[],"source":["# Retrieve only the texts identified as English language by both cld3 and fasttext langid\n","df = df[df.language == 'en']"]},{"cell_type":"markdown","metadata":{},"source":["Next, it can be quite interseting to check Nan values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# find which column contain nan value\n","df.columns[df.isna().any()].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get all rows that contain NaN values\n","df_nan = df[df.isna().any(axis=1)]\n","df_nan"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Number of untitled song:', len(df[df.isna().any(axis=1)]))"]},{"cell_type":"markdown","metadata":{},"source":["Insofar as the title of the music is not to be taken into account in the learning of the topic modeling algorithms but But the titles can be related to the topics in the next phase of analysis and the low number of songs without any title, I decide to delete this data for the moment."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:58.036255Z","iopub.status.busy":"2023-05-22T11:05:58.035853Z","iopub.status.idle":"2023-05-22T11:05:58.663112Z","shell.execute_reply":"2023-05-22T11:05:58.661698Z","shell.execute_reply.started":"2023-05-22T11:05:58.036219Z"},"trusted":true},"outputs":[],"source":["# Delete rows containing NaN values\n","df = df.dropna()\n","len(df)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we also try to check for None values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df[df.isnull().any(axis=1)]"]},{"cell_type":"markdown","metadata":{},"source":["No None values in this dataframe.\n","\n","Afterwards, let's look at the year variable, which is one of the important variables to take into account in our analysis because we want to extract the topics by decades."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["years = df.year.unique()\n","print(years)\n","\n","print('Number of unique years: ',len(years))"]},{"cell_type":"markdown","metadata":{},"source":["We firstly want to know if the year variable format is suitable. It is highly likely that year are sometimes downsized (example : 92 instead of 1992).\n","Let's display the tag distribution for music with a release year below 215."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_tag = df[df['year'] < 215].groupby(['tag']).size().reset_index(name='count')\n","\n","fig = px.pie(df_tag, names=\"tag\", values=\"count\", title = \"Outlier tag distribution\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["It is rather surprising to observe that the majority style of music of this period (< 215) is rap music knowing that this style is known for the current emerging style. Of course, among this data their is a important part of outlier year."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Extract the pieces of music of type 'rap' lower than the year 215\n","df_rap = df[(df['year'] < 215) & (df['tag'] == 'rap')]\n","df_rap.sort_values(by='views',ascending=False).head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_rap[df_rap['artist'] == 'Kanye East']"]},{"cell_type":"markdown","metadata":{},"source":["If we search the release date of this track on google, we can find a release date from 4 May 2021 on the [Genius website](https://genius.com/Kanye-east-the-secrets-of-dababy-lyrics). Given the year that we find in our table and real one, we can assume some issue about the date format (1 instead 2021).\n","\n","After few research on genius website, the most viewed songs of this above displayed list seems to be released on 2021 but more views decrease harder is the interpretation of date.\n","\n","Let's check the second most popular tag 'pop' in this retrieve outliers data :"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Extract the pieces of music of type 'rap' lower than the year 215\n","df_pop = df[(df['year'] < 215) & (df['tag'] == 'pop')]\n","df_pop.sort_values(by='views',ascending=False).head(20)"]},{"cell_type":"markdown","metadata":{},"source":["The titles recovered seem to be for the most part recent sounds, not very popular with a bad indexation of the years.\n","\n","A case-by-case pre-processing of the data is too tedious compared to the amount of data to be processed. We will only use data with correctly formatted dates."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:58.665931Z","iopub.status.busy":"2023-05-22T11:05:58.664883Z","iopub.status.idle":"2023-05-22T11:05:58.813157Z","shell.execute_reply":"2023-05-22T11:05:58.811886Z","shell.execute_reply.started":"2023-05-22T11:05:58.665864Z"},"trusted":true},"outputs":[],"source":["df = df[(df.year >= 1960) & (df.year < 2023)]\n","len(df)"]},{"cell_type":"raw","metadata":{},"source":["def filter_wrong_format(x):\n","    if x > 100 and x <= 2023:\n","        fyear = x\n","    else:\n","        if x >= 0 and x <= 23:\n","            fyear = x + 2000\n","        elif x >= 23 and x < 100:\n","            fyear = x + 1900\n","        else:\n","            raise Exception(\n","                \"Something seems wrong with filters value : {}\"\n","                .format(x))\n","    return fyear # return the fixed year\n","\n","assert filter_wrong_format(2000) == 2000\n","assert filter_wrong_format(9) == 2009\n","assert filter_wrong_format(71) == 1971\n","\n"]},{"cell_type":"raw","metadata":{},"source":["# replace the column name\n","df['year'] = df['year'].apply(filter_wrong_format)\n","df['year'].unique()\n","len(df)"]},{"cell_type":"markdown","metadata":{},"source":["We wish to analyze the texts by decade then let's add a decade column."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:58.815206Z","iopub.status.busy":"2023-05-22T11:05:58.814855Z","iopub.status.idle":"2023-05-22T11:05:59.653965Z","shell.execute_reply":"2023-05-22T11:05:59.652771Z","shell.execute_reply.started":"2023-05-22T11:05:58.815171Z"},"trusted":true},"outputs":[],"source":["import math\n","\n","df['decade'] = df['year'].map(lambda x : int(math.trunc(x / 10) * 10))\n","\n","df.sort_values(by = 'year').head(20)"]},{"cell_type":"markdown","metadata":{},"source":["# Data vizualisation\n","\n","Let's do some vizualisation to get a better understanding of our data. As we saw on previous distribution graphs over years, more titles have been recorded over the last 2 decades."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:59.656466Z","iopub.status.busy":"2023-05-22T11:05:59.655516Z","iopub.status.idle":"2023-05-22T11:05:59.804029Z","shell.execute_reply":"2023-05-22T11:05:59.802754Z","shell.execute_reply.started":"2023-05-22T11:05:59.656425Z"},"trusted":true},"outputs":[],"source":["# barplot by decade\n","def barplot_by_decade(df):\n","\n","    # groupby decade\n","    df_d = df.groupby(['decade']).size().reset_index(name='count')\n","\n","    # create the figure\n","    fig = go.Figure()\n","\n","    fig.add_bar(\n","        x=df_d.decade,\n","        y=df_d['count'],\n","        showlegend=False)\n","\n","    fig.add_scatter(\n","            x=df_d.decade,\n","            y=df_d[\"count\"],\n","            mode=\"markers+lines\",\n","            name=\"trend\",\n","            showlegend=False)\n","\n","    fig.update_layout(\n","            title = \"Music release over years\",\n","            xaxis_title=\"decade\",\n","            yaxis_title=\"release\")\n","    return fig\n","\n","# build and display\n","fig = barplot_by_decade(df)\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:08:57.151567Z","iopub.status.busy":"2023-05-22T11:08:57.15114Z","iopub.status.idle":"2023-05-22T11:08:57.248993Z","shell.execute_reply":"2023-05-22T11:08:57.247582Z","shell.execute_reply.started":"2023-05-22T11:08:57.151529Z"},"trusted":true},"outputs":[],"source":["# compute tag frequencies by decade\n","df_pies_d = df.groupby(['decade','tag']).size().reset_index(name='count')\n","df_pies_d[df_pies_d.decade == 1960]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:00.445517Z","iopub.status.busy":"2023-05-22T11:09:00.445064Z","iopub.status.idle":"2023-05-22T11:09:00.713566Z","shell.execute_reply":"2023-05-22T11:09:00.7122Z","shell.execute_reply.started":"2023-05-22T11:09:00.445475Z"},"trusted":true},"outputs":[],"source":["from plotly.subplots import make_subplots\n","\n","# create en make subplot\n","fig = make_subplots(rows=3, cols=3,\n","                    specs=[\n","                        [{'type':'domain'}\n","                        for i in range(1,4)] for i in range(1,4)\n","                    ])\n","decades = df_pies_d.decade.unique().tolist()\n","for i in range(0,3):\n","    for k in range(0,3):\n","        decade = decades[i*3 + k]\n","        # group by decade\n","        df_p = df_pies_d[df_pies_d.decade == decade]\n","        # add figure\n","        fig.add_trace(go.Pie(labels=df_p.tag, values=df_p['count'], name=decade), i+1, k+1)\n","        # add annotation\n","        fig.add_annotation(arg=dict(\n","            text=decade, x=k*0.375 + 0.125,\n","            y=-i*0.3927 + 0.90, font_size=10,\n","            showarrow=False))\n","        if (i*3 + k) == 6:\n","            break\n","\n","\n","# Use `hole` to create a donut-like pie chart\n","fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n","\n","fig.update_layout(\n","    title_text=\"Tags proportions by decades\"\n","    # Add annotations in the center of the donut pies.\n","    #annotations=[dict(text=decade, x=k*0.375+0.125, y= -i*0.125+0.90, font_size=10, showarrow=False)\n","     #           for k, decade in enumerate(decades) for i in range(0,4)]\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["The different pie charts show us an evolution of the different proportions of music styles by decades from 1960 to today. In the sixties the most listed music style is the **pop music**. Then comes an emergence of a rising style: the **rock**. This one increases with the decades until it overtakes the pop music during the decades 90 and 2000. The two styles come to balance thereafter facing the meteoric rise of the most listed genre in our current decade, namely **rap**. The evolution of the proportions recover can give us a rather precise idea of the most popular styles of their times. However, it is important to remember that the data still has some bias, Genius being a crowdsourced tool created during the last decade and at the base only as a lexical translator of rap music, it is normal to find a large amount of data from this period and especially from this style there."]},{"cell_type":"markdown","metadata":{},"source":["The different pie charts show us an evolution of the different proportions of music styles by decades from 1960 to today. In the sixties the most listed music style is the **pop music**. Then comes an emergence of a rising style: the **rock**. This one increases with the decades until it overtakes the pop music during the decades 90 and 2000. The two styles come to balance thereafter facing the meteoric rise of the most listed genre in our current decade, namely **rap**. The evolution of the proportions recover can give us a rather precise idea of the most popular styles of their times. However, it is important to remember that the data still has some bias, Genius being a crowdsourced tool created during the last decade and at the base only as a lexical translator of rap music, it is normal to find a large amount of data from this period and especially from this style there.\n","\n","## Text preprocessing\n","\n","After the visualisation part let's focus more on the main data which are the lyrics."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.iloc[0][\"lyrics\"]"]},{"cell_type":"markdown","metadata":{},"source":["There is many undesirable characters like the line breaker '\\n', figures or square, curly and simple brackets. So let's clean this data with regular expressions."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:04.712168Z","iopub.status.busy":"2023-05-22T11:09:04.711677Z","iopub.status.idle":"2023-05-22T11:09:04.720611Z","shell.execute_reply":"2023-05-22T11:09:04.71899Z","shell.execute_reply.started":"2023-05-22T11:09:04.712122Z"},"trusted":true},"outputs":[],"source":["import re\n","from numpy.random import randint\n","\n","def clean_text(text):\n","    # remove \\n\n","    text = text.replace('\\n', ' ')\n","    # remove punctuation\n","    text = re.sub(r'[,\\.!?]', '', text)\n","    #removing text in square braquet\n","    text = re.sub(r'\\[.*?\\]', ' ', text)\n","    #removing numbers\n","    text = re.sub(r'\\w*\\d\\w*',' ', text)\n","    #removing bracket\n","    text = re.sub(r'[()]', ' ', text)\n","    # convert all words in lower case\n","    text = text.lower()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get the results of data cleaning\n","cleaned_text = df[\"lyrics\"].apply(clean_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-22T08:48:48.810789Z","iopub.status.idle":"2023-05-22T08:48:48.811846Z","shell.execute_reply":"2023-05-22T08:48:48.811583Z","shell.execute_reply.started":"2023-05-22T08:48:48.811547Z"},"trusted":true},"outputs":[],"source":["docs = cleaned_text.to_list()\n","docs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# update dataframe\n","df.update(cleaned_text)\n","df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-22T08:48:48.8135Z","iopub.status.idle":"2023-05-22T08:48:48.814309Z","shell.execute_reply":"2023-05-22T08:48:48.813908Z","shell.execute_reply.started":"2023-05-22T08:48:48.813854Z"},"trusted":true},"outputs":[],"source":["df.iloc[0]['lyrics']"]},{"cell_type":"markdown","metadata":{},"source":["That's better! The libraries that we will use later to perform topic modeling usually provide preprocessing but it is always good to have control over what we manipulate.\n","\n","# Topic modeling\n","\n","- [LDA (latent dirichlet allocation)](https://fr.wikipedia.org/wiki/Allocation_de_Dirichlet_latente) are the common way to do topic modeling in the few last years, it works and it's quite easy to use with common python library like [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html).\n","- [BERTopic](https://maartengr.github.io/BERTopic/index.html) seems to be one of the best technic this day to perform topic modeling. It combine the leverage of [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) the famous language model with [c-TF-IDF](https://maartengr.github.io/BERTopic/api/ctfidf.html) tansformer.\n","\n","In order to reach my first dead line (17/03/2023), I didn't have enough time to run BerTopic algorithm.\n","\n","\n","## Define default tokenizer and Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:28.56074Z","iopub.status.busy":"2023-05-22T11:09:28.560261Z","iopub.status.idle":"2023-05-22T11:09:29.298872Z","shell.execute_reply":"2023-05-22T11:09:29.297329Z","shell.execute_reply.started":"2023-05-22T11:09:28.560681Z"},"trusted":true},"outputs":[],"source":["import spacy\n","\n","print(\"set gpu: \", spacy.prefer_gpu())\n","\n","# small model /!\\ take the bigger one for Kaggle\n","new_nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","metadata":{},"source":["It could be difficult to process all this data on my computer or Kaggle. The memory will quickly be overwhelmed. I will work with a sample of our previously load data in order to avoid memory overload."]},{"cell_type":"raw","metadata":{},"source":["# create weights series\n","freq = df.groupby('decade')['decade'].transform('count')\n","\n","# set sample frac of the data\n","prop = 0.3\n","n_samples = prop * len(df)\n","n_factors = len(freq.unique())\n","\n","df['freq'] = freq.apply(lambda row : n_samples / (n_factors * row))\n","print(df.freq.unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:32.57195Z","iopub.status.busy":"2023-05-22T11:09:32.570518Z","iopub.status.idle":"2023-05-22T11:09:32.580306Z","shell.execute_reply":"2023-05-22T11:09:32.579139Z","shell.execute_reply.started":"2023-05-22T11:09:32.57189Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","\n","def sample(balanced = True, data = df, prop = 0.2):\n","    if balanced:\n","        # compute the sorted decreasing parties frequencies\n","        decade_frequencies = Counter(data['decade']).most_common()\n","        print(decade_frequencies)\n","\n","        # retrieve the under represented class\n","        nb_under_class = decade_frequencies[-1][1]\n","\n","        # Return a random sample of items from each party following the under sampled number of class\n","        sample_df = data.groupby(\"decade\").sample(n = int(prop * nb_under_class), random_state = 20)\n","    else:\n","        # create sample df 1/3 of the actual loaded data\n","        sample_df = df.sample(frac = prop)\n","    return sample_df\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:53.115734Z","iopub.status.busy":"2023-05-22T11:09:53.115261Z","iopub.status.idle":"2023-05-22T11:09:54.266326Z","shell.execute_reply":"2023-05-22T11:09:54.265089Z","shell.execute_reply.started":"2023-05-22T11:09:53.115679Z"},"trusted":true},"outputs":[],"source":["# sample the data\n","sdf = sample()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:55.952293Z","iopub.status.busy":"2023-05-22T11:09:55.951869Z","iopub.status.idle":"2023-05-22T11:09:55.973056Z","shell.execute_reply":"2023-05-22T11:09:55.972081Z","shell.execute_reply.started":"2023-05-22T11:09:55.952254Z"},"trusted":true},"outputs":[],"source":["# check the distribution of the sample\n","barplot_by_decade(sdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:10:01.538657Z","iopub.status.busy":"2023-05-22T11:10:01.538085Z","iopub.status.idle":"2023-05-22T11:10:02.646844Z","shell.execute_reply":"2023-05-22T11:10:02.645465Z","shell.execute_reply.started":"2023-05-22T11:10:01.538602Z"},"trusted":true},"outputs":[],"source":["# get the results of data cleaning\n","cleaned_text = sdf[\"lyrics\"].apply(clean_text)\n","# update dataframe\n","sdf.update(cleaned_text)\n","sdf.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:10:05.055569Z","iopub.status.busy":"2023-05-22T11:10:05.055134Z","iopub.status.idle":"2023-05-22T11:10:05.063868Z","shell.execute_reply":"2023-05-22T11:10:05.062504Z","shell.execute_reply.started":"2023-05-22T11:10:05.05553Z"},"trusted":true},"outputs":[],"source":["# default preprocessing\n","def preprocess(text, nlp = new_nlp):\n","\n","    #TOKENISATION\n","    tokens =[]\n","    for token in nlp(text):\n","        tokens.append(token)\n","\n","    #REMOVING STOP WORDS\n","    spacy_stopwords = new_nlp.Defaults.stop_words\n","    sentence =  [word for word in tokens if word.text.isalpha() and word.text not in spacy_stopwords]\n","\n","    #LEMMATISATION\n","    sentence = [word.lemma_ for word in sentence]\n","\n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["## Vizualise most frequent words over decades"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:10:06.836787Z","iopub.status.busy":"2023-05-22T11:10:06.836321Z","iopub.status.idle":"2023-05-22T11:10:06.871445Z","shell.execute_reply":"2023-05-22T11:10:06.870292Z","shell.execute_reply.started":"2023-05-22T11:10:06.836742Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","class TermsDocumentsMatrix():\n","    \n","    def __init__(self, sdf, decades=[1960, 1970], colorscale = 'Plotly3'):\n","        # vectorizer on the sample lyrics\n","        self.__vectorizer = CountVectorizer(tokenizer = preprocess)\n","        # fit and transform the data\n","        self.__data_vectorized = self.__vectorizer.fit_transform(\n","            tqdm(sdf['lyrics'].loc[sdf['decade'].isin(decades)])\n","        )\n","        # get decades informations\n","        self.__decades = sdf['decade'].loc[sdf['decade'].isin(decades)].reset_index(drop=True)\n","        self.__unique_decades = decades\n","        # get colorscale template\n","        self.__colorscale = colorscale\n","    \n","    def get_tdmatrix(self):\n","        \n","        # compute a Matrix terms document by decades\n","        df_bw = pd.DataFrame(self.__data_vectorized.toarray(),\n","                    columns = self.__vectorizer.get_feature_names_out())\n","        \n","        # check the length\n","        if len(df_bw) != len(self.__decades):\n","            raise Exception('Not the same size')\n","        \n","        # concatenate decade\n","        df_bw['decade'] = self.__decades\n","        \n","        # check NaN values\n","        if len(df_bw.columns[df_bw.isna().any()].tolist()) != 0:\n","            raise Exception('Decade got Nan values')\n","\n","        return df_bw\n","    \n","    def get_tdm_by_decade(self, decade):\n","        \n","        if decade not in self.__unique_decades:\n","            raise Exception(\"{} doesn't appear in the decades list\".format(decade))\n","        \n","        # compute a Matrix terms document by decades (bag of words format)\n","        df_bw = pd.DataFrame(self.__data_vectorized.toarray(),\n","                    columns = self.__vectorizer.get_feature_names_out())\n","        \n","        # check the length\n","        if len(df_bw) != len(self.__decades):\n","            raise Exception('Not the same size')\n","        \n","        # concatenate decade\n","        df_bw['decade'] = self.__decades\n","        \n","        # check NaN values\n","        if len(df_bw.columns[df_bw.isna().any()].tolist()) != 0:\n","            raise Exception('Decade got Nan values')\n","        \n","        # select suitable decade\n","        df_bw = df_bw[df_bw['decade'] == decade]\n","        \n","        return df_bw\n","    \n","    def most_freq_terms(self, n_rows = 1, n_cols = 2, n_terms = 10):\n","        \n","        # create the document terms matrix\n","        df_bw = self.get_tdmatrix()\n","        \n","        # create en make subplot\n","        fig = make_subplots(rows=n_rows, cols=n_cols,\n","                            x_title = 'number of occurrences',\n","                            y_title = 'terms',\n","                            subplot_titles = self.__unique_decades)\n","        \n","        for i in range(0,n_rows):\n","            for k in range(0,n_cols):\n","                if (i*n_rows + k) == len(self.__unique_decades):\n","                    break\n","                \n","                # get the decade\n","                decade = self.__unique_decades[i*n_rows + k]\n","            \n","                #select the suitable decade and delete decade column\n","                df_decade = df_bw.loc[df_bw.decade == decade, df_bw.columns != 'decade']\n","                \n","                # compute terms frequencies by decade\n","                terms_freq = df_decade.sum().sort_values(ascending = False)\n","            \n","                # total number of terms occurences\n","                total_terms = terms_freq.values\n","                \n","                # add figure\n","                fig.add_trace(go.Bar(y=terms_freq.index.tolist()[:n_terms][::-1],\n","                                     x=total_terms[:n_terms][::-1],\n","                                     name=decade,\n","                                     orientation='h', showlegend = False,\n","                                    marker = dict(color = total_terms,\n","                                                  colorscale=self.__colorscale)),\n","                              i+1, k+1)\n","        return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# first decades\n","tdm = TermsDocumentsMatrix(sdf, decades = [1960, 1970, 1980, 1990],\n","                           colorscale = 'Plasma')\n","\n","# display bar charts of most frequent terms\n","tdm.most_freq_terms(n_rows = 2, n_cols = 2, n_terms = 15)"]},{"cell_type":"markdown","metadata":{},"source":["According to the bar graphs displayed above, a group of words seems to recur on each decade: Love, know, go, feel ... Words that seem to relate to the popular song that can talk about love. This is consistent with our previous analysis from the pie charts showing the proportions of musical styles across time. We also notice an important presence of onomatopoeia like yeah or oh."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# first decades\n","tdm = TermsDocumentsMatrix(sdf, decades = [2000, 2010, 2020],\n","                          colorscale = 'Plasma')\n","\n","# most frequent terms\n","tdm.most_freq_terms(n_rows = 2, n_cols = 2, n_terms = 15)"]},{"cell_type":"markdown","metadata":{},"source":["We get similar results on this second decade with similar high occurrence words. We see a greater amount of onomatopoeia in the current decade. We can explain this by an emergence of the rap music style on this current and last decade. There is in this style of music a very used process, the 'ad-libs'. They are sounds, words or onomatopoeias that the artists pronounce sometimes between two verses or at the end of a sentence to give more impact to their text and to dynamize the atmosphere of a title. This may explain the greater presence of onomatopoeia in the lyrics of this decade.\n","\n","\n","## Topic modeling with LDA\n","\n","LDA is a common technic use in topic modeling, we firstly process basic preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T14:35:34.025931Z","iopub.status.busy":"2023-05-22T14:35:34.025087Z","iopub.status.idle":"2023-05-22T14:35:34.034193Z","shell.execute_reply":"2023-05-22T14:35:34.032577Z","shell.execute_reply.started":"2023-05-22T14:35:34.02586Z"},"trusted":true},"outputs":[],"source":["# utils\n","from datetime import datetime\n","import logging\n","\n","gensim_log = '/kaggle/working/sample.log'\n","\n","#initiate log file\n","logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',\n","                    level=logging.INFO,\n","                   force = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T14:40:59.195936Z","iopub.status.busy":"2023-05-22T14:40:59.195299Z","iopub.status.idle":"2023-05-22T14:40:59.246425Z","shell.execute_reply":"2023-05-22T14:40:59.244966Z","shell.execute_reply.started":"2023-05-22T14:40:59.195888Z"},"trusted":true},"outputs":[],"source":["# gensim\n","from gensim.models import CoherenceModel\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.ldamulticore import LdaMulticore\n","from gensim.corpora.dictionary import Dictionary\n","from gensim.test.utils import datapath\n","from tqdm import tqdm\n","\n","# utils\n","from datetime import datetime\n","import logging\n","\n","# dashboards\n","import pyLDAvis\n","import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","\n","# TSNE dependencies\n","from sklearn.manifold import TSNE\n","from bokeh.plotting import figure, output_file, show\n","from bokeh.models import Label\n","from bokeh.io import output_notebook\n","import numpy as np\n","import matplotlib.colors as mcolors\n","\n","# utils\n","def parse_logfile(path_log):\n","    matcher = re.compile(r'(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity')\n","    likelihoods = []\n","    with open(path_log, 'w') as source:\n","        for line in source:\n","            match = matcher.search(line)\n","            if match:\n","                likelihoods.append(float(match.group(1)))\n","    return likelihoods\n","\n","\n","class LDATopicModeling():\n","    \n","    def __init__(self, df = sdf,\n","                 decade = 1960,\n","                 directory = \"/kaggle/working/models/\",\n","                 existing = False,\n","                 n_topics = 10,\n","                 worker_nodes = None,\n","                lang_preprocess = preprocess,\n","                cross_valid = False,\n","                epochs = 30):\n","        # Apply preprocessing on decade data\n","        self.__documents = df.loc[df.decade == decade, 'lyrics'].apply(lang_preprocess)\n","            \n","        # Create a corpus from a list of texts\n","        self.__id2word = Dictionary(self.__documents.tolist())\n","        self.__corpus = [self.__id2word.doc2bow(doc) for doc in self.__documents.tolist()]\n","        \n","        #training\n","        if os.path.isfile(existing):\n","            # Load a potentially pretrained model from disk.\n","            self.model = LdaModel.load(temp_file)\n","            self.__cv_results = None # no cross_valid\n","            self.__n_topics = n_topics\n","        elif not cross_valid:\n","            self.model = LdaMulticore(\n","                corpus=tqdm(self.__corpus),\n","                id2word=self.__id2word,\n","                num_topics=n_topics,\n","                workers=worker_nodes,\n","                passes=epochs)\n","            self.__likelihood = parse_logfile(gensim_log)\n","            self.__n_topics = n_topics\n","            self.__cv_results = None\n","        else: # cross validation\n","            \n","            # hyperparameter\n","            alpha = []#np.arange(0.01, 1, 0.3).tolist()\n","            alpha.append('symmetric')\n","            alpha.append('asymmetric')\n","            \n","            # hyperparameter\n","            eta = []#np.arange(0.01, 1, 0.3).tolist()\n","            eta.append('symmetric')\n","            \n","            # compute results of the cross_validation\n","            cv_results = {\n","                 'topics': [],\n","                 'alpha': [],\n","                 'eta': [],\n","                 'coherence': []\n","            }\n","            \n","            # topic range\n","            topic_range = range(2, n_topics+1)\n","            \n","            # prevent the computation time\n","            total=(len(eta)*len(alpha)*len(topic_range))\n","            print(\"total lda computation: \",total)\n","            model_list = []\n","            \n","            for k in topic_range:\n","                for a in alpha:\n","                    for e in eta:\n","                        \n","                        # train the model\n","                        model = LdaMulticore(\n","                            corpus=self.__corpus,\n","                            id2word=self.__id2word,\n","                            num_topics=k,\n","                            workers=worker_nodes,\n","                            passes=epochs,\n","                            alpha=a,\n","                            eta=e)\n","                        \n","                        # compute coherence\n","                        cv = CoherenceModel(\n","                            model=model,\n","                            texts=self.__documents,\n","                            dictionary=self.__id2word,\n","                            coherence='c_v')\n","                        \n","                        print('coherence: {}\\nalpha: {}\\neta: {}\\ntopic: {}'.format(\n","                            cv.get_coherence(), a, e, k))\n","                        \n","                         # Save the model results\n","                        cv_results['topics'].append(k)\n","                        cv_results['alpha'].append(a)\n","                        cv_results['eta'].append(e)\n","                        cv_results['coherence'].append(cv.get_coherence())\n","                        model_list.append(model)\n","            # retrieve index of the highest coherence model\n","            best_index = np.argmax(cv_results['coherence'])\n","            \n","            # choose the model given the best coherence\n","            self.model = model_list[best_index]\n","            \n","            # save results as attribute\n","            self.__cv_results = cv_results\n","            \n","            self.__n_topics = cv_results['topics'][best_index]\n","                                    \n","            # logging doesn't work on Kaggle\n","            #self.__likelihood = parse_logfile()\n","        # directory path\n","        self.__directory = directory\n","    \n","    # getters\n","    @property\n","    def get_id2word(self):\n","        return self.__id2word\n","    \n","    @property\n","    def get_corpus(self):\n","        return self.__corpus\n","    \n","    @property\n","    def get_likelihood(self):\n","        return self.__likelihood\n","    \n","    @property\n","    def get_cv_results(self):\n","        return pd.DataFrame(self.__cv_results) if self.__cv_results else None\n","    \n","    def plot_coherence(self, metric = 'alpha'):\n","        \"\"\"metric(str): alpha or eta\n","        \"\"\"\n","        if self.__cv_results is None:\n","            raise Exception('No cross validation available')\n","        \n","        # get the dataframe\n","        df_res = self.get_cv_results\n","                                    \n","        # groupby by metric\n","        grouped = df_res.groupby(metric)\n","        # create the layout\n","        fig = go.Figure()\n","        for level, df in grouped:\n","            fig.add_trace(\n","                go.Scatter(\n","                    x=df.topics,\n","                    y=df.coherence,\n","                    mode='lines+markers',\n","                    name=str(level)\n","                )\n","            )\n","        fig.update_layout(\n","            title = \"coherence over topics by\",\n","            xaxis_title=\"topic\",\n","            yaxis_title=\"coherence\")\n","        return fig\n","                                    \n","                                    \n","    def save_current_model(self):\n","        # retrieve time\n","        now = datetime.now()\n","        # create the directory if it doesn't exist\n","        try:\n","            os.makedirs(directory + now.strftime(\"%d%m%Y_%H%M%S\"))\n","        except:\n","            pass\n","        # Save model to disk.\n","        temp_file = datapath(directory + now.strftime(\"%d%m%Y_%H%M%S\") + '/model')\n","        \n","        self.model.save(temp_file)\n","\n","    def get_perplexity(self):\n","        return self.model.log_perplexity(self.__corpus)\n","    \n","    def get_coherence(self):\n","        coherence_model_lda = CoherenceModel(\n","            model=self.model,\n","            texts=self.__documents,\n","            dictionary=self.__id2word,\n","            coherence='c_v')\n","        return coherence_model_lda.get_coherence()\n","    \n","    \n","    # data vizualisation\n","    def dashboard_LDAvis(self):\n","        # some basic dataviz\n","        pyLDAvis.enable_notebook()\n","        vis = pyLDAvis.gensim.prepare(self.model, self.__corpus,\n","                                      dictionary = self.model.id2word)\n","        return vis\n","        \n","    def plot_tsne(self, components = 2):\n","        # n-1 rows each is a vector with i-1 posisitons, where n the number of documents\n","        # i the topic number and tmp[i] = probability of topic i\n","        topic_weights = []\n","        for row_list in self.model[self.get_corpus]:\n","            tmp = np.zeros(self.__n_topics)\n","            for i, w in row_list:\n","                tmp[i] = w\n","            topic_weights.append(tmp)\n","\n","        # Array of topic weights    \n","        arr = pd.DataFrame(topic_weights).fillna(0).values\n","        \n","        # Keep the well separated points\n","        # filter documents with highest topic probability given lower bown (optional)\n","        # arr = arr[np.amax(arr, axis=1) > 0.35]\n","      \n","        # Dominant topic number in each doc (to compute color)\n","        topic_num = np.argmax(arr, axis=1)\n","\n","        # tSNE Dimension Reduction\n","        tsne_model = TSNE(n_components=components, verbose=1, init='pca')\n","        tsne_lda = tsne_model.fit_transform(arr)\n","        \n","        mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n","        \n","        # components\n","        if components == 2:\n","            fig = go.Figure(\n","                go.Scatter(x=tsne_lda[:,0],\n","                        y=tsne_lda[:,1],\n","                        marker_color=mycolors[topic_num],\n","                        mode='markers',\n","                        name='Tsne'))\n","            fig.update_layout(\n","                title = \"t-SNE Clustering of {} LDA Topics\".format(self.__n_topics),\n","                xaxis_title=\"x\",\n","                yaxis_title=\"y\",\n","                autosize=False,\n","                width=900,\n","                height=700)\n","        elif components == 3:\n","            fig = go.Figure(\n","                go.Scatter3d(\n","                        x=tsne_lda[:,0],\n","                        y=tsne_lda[:,1],\n","                        z=tsne_lda[:,2],\n","                        marker_color=mycolors[topic_num],\n","                        mode='markers',\n","                        name='Tsne'))\n","            fig.update_layout(\n","                title = \"t-SNE Clustering of {} LDA Topics\".format(self.__n_topics),\n","                xaxis_title=\"x\",\n","                yaxis_title=\"y\")\n","        else:\n","            raise Exception(\"Components exceed covered numbers : {}\".format(components))\n","        return fig\n","        \n","        #plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(self.__n_topics), \n","        #             plot_width=900, plot_height=700)\n","        #plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n","        #show(plot)\n","        \n","    def plot_likelihood(self):\n","        fig = go.Figure(\n","            go.Scatter(x=[i for i in range(0,50)], y=self.__likelihood[-50:],\n","                       mode='lines',\n","                       name='lines'))\n","        fig.update_layout(\n","            title = \"Likelihood over passes\",\n","            xaxis_title=\"Likekihood\",\n","            yaxis_title=\"passes\")\n","        return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T14:39:26.380282Z","iopub.status.busy":"2023-05-22T14:39:26.379741Z","iopub.status.idle":"2023-05-22T14:39:26.43427Z","shell.execute_reply":"2023-05-22T14:39:26.432492Z","shell.execute_reply.started":"2023-05-22T14:39:26.380235Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/working/sample.log',\"w\") as source:\n","    for line in source:\n","        match = matcher.search(line)\n","        if match:\n","            likelihoods.append(float(match.group(1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T14:41:07.657982Z","iopub.status.busy":"2023-05-22T14:41:07.657521Z","iopub.status.idle":"2023-05-22T14:41:53.66553Z","shell.execute_reply":"2023-05-22T14:41:53.663734Z","shell.execute_reply.started":"2023-05-22T14:41:07.657942Z"},"trusted":true},"outputs":[],"source":["# create my model\n","lda_model = LDATopicModeling(df = sdf, decade = 1990)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["a = [elem for elem in lda_model.model[lda_model.get_corpus]]\n","for i in a:\n","    if len(i) == 1:\n","        print(\"la taille dépasse 1\")\n","        print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print([i for i in lda_model.model[lda_model.get_corpus]])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:44:43.190714Z","iopub.status.idle":"2023-05-22T13:44:43.19133Z","shell.execute_reply":"2023-05-22T13:44:43.19105Z","shell.execute_reply.started":"2023-05-22T13:44:43.191021Z"},"trusted":true},"outputs":[],"source":["print([i for i in lda_model.get_corpus])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-22T13:44:43.1927Z","iopub.status.idle":"2023-05-22T13:44:43.193254Z","shell.execute_reply":"2023-05-22T13:44:43.193051Z","shell.execute_reply.started":"2023-05-22T13:44:43.193024Z"},"trusted":true},"outputs":[],"source":["# print the result\n","lda_model.model.print_topics()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-05-22T14:14:08.503472Z","iopub.status.idle":"2023-05-22T14:14:08.50401Z","shell.execute_reply":"2023-05-22T14:14:08.503805Z","shell.execute_reply.started":"2023-05-22T14:14:08.503779Z"},"trusted":true},"outputs":[],"source":["lda_model.plot_tsne(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T14:37:31.065413Z","iopub.status.busy":"2023-05-22T14:37:31.064512Z","iopub.status.idle":"2023-05-22T14:37:31.07544Z","shell.execute_reply":"2023-05-22T14:37:31.073912Z","shell.execute_reply.started":"2023-05-22T14:37:31.065358Z"},"trusted":true},"outputs":[],"source":["lda_model.get_likelihood\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:11:40.029365Z","iopub.status.busy":"2023-05-22T11:11:40.028924Z","iopub.status.idle":"2023-05-22T11:11:43.020206Z","shell.execute_reply":"2023-05-22T11:11:43.018627Z","shell.execute_reply.started":"2023-05-22T11:11:40.029328Z"},"trusted":true},"outputs":[],"source":["lda_model.dashboard_LDAvis()"]},{"cell_type":"markdown","metadata":{},"source":["Let's perform lda for each decade."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:12:26.852454Z","iopub.status.busy":"2023-05-22T11:12:26.851963Z","iopub.status.idle":"2023-05-22T11:12:26.866156Z","shell.execute_reply":"2023-05-22T11:12:26.864914Z","shell.execute_reply.started":"2023-05-22T11:12:26.852408Z"},"trusted":true},"outputs":[],"source":["# LDA Topic Modeling by decade\n","class LDAPipeline():\n","    \n","    def __init__(self,\n","                 prep = preprocess,\n","                 cv = False,\n","                 decades = [\n","        1960, 1970, 1980, 1990, 2000, 2010, 2020\n","    ]):\n","        self.models = {\n","            decade : LDATopicModeling(\n","                decade = decade,\n","                lang_preprocess = prep,\n","                epochs = 10,\n","                cross_valid = cv) for decade in decades}\n","        \n","    def get_metrics(self):\n","        # compute metrics\n","        metrics = {\n","                 'decade': [],\n","                 'coherence': [],\n","                 'perplexity': []\n","        }\n","        for decade, model in self.models.items():\n","            metrics['decade'].append(decade)\n","            metrics['coherence'].append(model.get_coherence())\n","            metrics['perplexity'].append(model.get_perplexity())\n","        # create the dataframe\n","        df_m = pd.DataFrame(metrics)\n","        df_m.set_index('decade')\n","        return df_m\n","        \n","        \n","    def lda_info(self, decade):\n","        lda_model = self.models[decade]\n","\n","        print(\"Perplexity: \", lda_model.get_perplexity())\n","        print(\"Coherence: \", lda_model.get_coherence())\n","        lda_model.plot_tsne()\n","        return lda_model.dashboard_LDAvis()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_models = LDAPipeline()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_models.get_metrics()"]},{"cell_type":"markdown","metadata":{},"source":["Display information for each decade."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1960 = lda_models.lda_info(1960)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1960"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1970 = lda_models.lda_info(1970)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1970"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1980 = lda_models.lda_info(1980)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1980"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1990 = lda_models.lda_info(1990)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_1990"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_2000 = lda_models.lda_info(2000)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_2000"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_2010 = lda_models.lda_info(2010)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_2010"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_2020 = lda_models.lda_info(2020)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_2020"]},{"cell_type":"markdown","metadata":{},"source":["Basic LDA model give us our baseline, we've got this default **perspicacity** and **coherence** score. Let's try to improve this to score and also our qualitative intuition about topic. As we saw topic seems really similar over the decade and it's quite difficult to retrieve some good topics given the representation we compute. \n","\n","# Improve the preprocessing\n","\n","In this part, we will try to create a pre-processing function that can take into account bigrams and trigrams and also allow to put aside the terms that could have been too recurrent in the previous part.\n","\n","## ngram recognition with gensim\n","\n","A way to improve lyrics comprehension is to use bigram and trigram with the help of phraser in gensim."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gensim\n","\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n","\n","# process in lyrics into words\n","data = sdf['lyrics'].tolist()\n","data_words = list(sent_to_words(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# display the result\n","data_words[0][:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import gensim\n","\n","# Build the bigram and trigram models\n","bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n","trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n","\n","# Faster way to get a sentence clubbed as a trigram/bigram\n","bigram_model = gensim.models.phrases.Phraser(bigram)\n","trigram_model = gensim.models.phrases.Phraser(trigram)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from numpy.random import randint, seed\n","\n","# set seed from numpy\n","seed(18)\n","\n","# draw the upper bound\n","upper_bound = randint(len(data_words))\n","\n","# display a random example\n","print('bigram model: ', bigram_model[data_words[upper_bound]])\n","print('\\ntrigram model: ', bigram_model[data_words[upper_bound]])\n","print('\\nsentence: ',' '.join(data_words[upper_bound]))"]},{"cell_type":"markdown","metadata":{},"source":["We are able to see some bigram and bigram, most of the time it's a words with its adjective or a group of onomatopeia.\n","\n","As previously explained, we would also like to set aside irrelevant terms that may have been recurrent in our last analysis. If we look at the previous bar charts, we can observe a significant amount of these last terms over all the decades. This is the case for example of like, know or yeah. Most of the time these terms qualified as uninteresting are verbs or onomatopoeias. Let's try to identify the less interesting ones by comparing the bar charts with the visualizations of pyLDAvis. We can notice a strong recurrence of the verbs like, know, come, get which are not necessarily relevant because they are found in most of the analyzed topics. We can also find onomatopoeias oh, yeah and la in most of the topics."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# list recurrent terms\n","recurrent_terms = {'like','know','come','get', 'got','go','to','oh','yeah','la'}\n","\n","\n","# default preprocessing\n","def ngram_preprocess(text, nlp = new_nlp,\n","                     bigram = bigram_model,\n","                     trigram = trigram_model,\n","                    new_stopwords = recurrent_terms):\n","    \n","    # perform basic preprocessing to transform sentence to list of words\n","    words = gensim.utils.simple_preprocess(text)\n","    \n","    # customize stopwords\n","    spacy_stopwords = new_nlp.Defaults.stop_words\n","    ext_stopwords = spacy_stopwords | new_stopwords # union of set\n","    \n","    #removing stop words\n","    no_stop_words = [word for word in words if word not in ext_stopwords]\n","    \n","    # perform bigram model\n","    bigram_words = bigram[no_stop_words]\n","    \n","    # perform trigram model\n","    trigram_words = trigram[bigram_words]\n","    \n","    # recreate the sentence\n","    sentence = ' '.join(trigram_words)\n","    \n","    #tokenization to get lemma\n","    tokens = [token for token in nlp(sentence)]\n","    \n","    #LEMMATISATION and filter alphanumeric characters\n","    sentence = [word.lemma_ for word in tokens if word.text.isalpha()]\n","\n","    return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test with the previously draw sentence\n","ngram_preprocess(' '.join(data_words[upper_bound]))[:10]"]},{"cell_type":"markdown","metadata":{},"source":["Let's try to rerun LDA with the new preprocessing function "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model = LDATopicModeling(decade = 2000, lang_preprocess = ngram_preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model.dashboard_LDAvis()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model.plot_tsne()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model = LDATopicModeling(decade = 2000,\n","                               lang_preprocess = ngram_preprocess,\n","                              cross_valid = True, epochs = 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model.plot_coherence('alpha')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model.plot_coherence('eta')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model.plot_tsne()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ngram_model.dashboard_LDAvis()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_models = LDAPipeline(prep = ngram_preprocess, cv = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_models.get_metrics()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1960 = lda_cv_models.lda_info(1960)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1960"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1970 = lda_cv_models.lda_info(1970)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1970"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1980 = lda_cv_models.lda_info(1980)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1980"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1990 = lda_cv_models.lda_info(1990)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_1990"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_2000 = lda_cv_models.lda_info(2000)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_2000"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_2010 = lda_cv_models.lda_info(2010)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_2010"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_2020 = lda_cv_models.lda_info(2020)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_2020"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lda_cv_models.get_metrics()"]},{"cell_type":"markdown","metadata":{"_kg_hide-input":true},"source":["## 2015 songs lyrics topic modelling\n","\n","Let's first retrieve the english song in 2015 (year of with max genius lyrics repertoried)."]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["df_2015_en = df[(df['year'] == 2015) & (df['language'] == 'en')]\n","df_2015_en.head()"]},{"cell_type":"markdown","metadata":{},"source":["Let's plot the tag distribution"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["df_tag = df_2015_en.groupby(['tag']).size().reset_index(name='count')\n","\n","fig = px.bar(df_tag, x=\"tag\", y=\"count\", color='tag')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"_kg_hide-input":true},"source":["The barplot below shows the frequency of each tag color by total views"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["# retrieve groups\n","total_tag_views = df_2015_en.groupby(['tag'])['views'].sum()\n","tag_freq = df_2015_en.groupby(['tag']).size()\n","\n","# create df\n","df_tag = pd.DataFrame({'count' : tag_freq,'total_views' :total_tag_views})\n","df_tag.reset_index(inplace = True, names = 'tag')\n","\n","# print the barplot\n","fig = px.bar(df_tag, x=\"tag\", y=\"count\", color='total_views')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"_kg_hide-input":true},"source":["Let's try topic modelling with top2vec library which is the easiest to start. But first let's filter the lyrics."]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["text = df_2015_en.lyrics.iloc[0]\n","text"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["text = text.replace('\\n', '')\n","text"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["import re\n","text = re.sub('[,\\.!?]', '', text)\n","text"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["text = text.lower()\n","text"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["def clean_text(text):\n","    # remove \\n\n","    text = text.replace('\\n', '')\n","    # remove punctuation\n","    text = re.sub('[,\\.!?]', '', text)\n","    #removing text in square braquet\n","    text = re.sub('\\[.*?\\]', ' ', text)\n","    #removing figures\n","    text = re.sub('\\w*\\d\\w*',' ', text)\n","    # convert all words in lower case\n","    text = text.lower()\n","    return text\n","\n","results = df_2015_en[\"lyrics\"].apply(clean_text)\n","\n","docs = results.to_list()\n","docs[:2]"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["import multiprocessing\n","multiprocessing.cpu_count()"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["from top2vec import Top2Vec\n","\n","model = Top2Vec(docs, speed = 'fast-learn', workers = multiprocessing.cpu_count())"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["import os\n","from datetime import datetime\n","\n","try:\n","    os.makedirs('models')\n","except:\n","    pass\n","\n","today = datetime.now().strftime(\"%d%m%Y\")\n","\n","path = \"models/\" + \"top2vec_\" + today\n","\n","model.save(path)\n","model = Top2Vec.load(path)"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model.get_num_topics()"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["topic_sizes, topic_nums = model.get_topic_sizes()\n","topic_sizes"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["topic_words, word_scores, topic_nums = model.get_topics(457)\n","topic_words[0]"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model.generate_topic_wordcloud(topic_nums[0])"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model.generate_topic_wordcloud(topic_nums[1])"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model.generate_topic_wordcloud(topic_nums[2])"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=0, num_docs=5)\n","for doc, score, doc_id in zip(documents, document_scores, document_ids):\n","    print(f\"Document: {doc_id}, Score: {score}\")\n","    print(\"-----------\")\n","    print(doc)\n","    print(\"-----------\")\n","    print()"]},{"cell_type":"markdown","metadata":{"_kg_hide-input":true},"source":["# 2000 song lyrics topic modelling"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["df_2000_en = df[(df['year'] == 2000) & (df['language'] == 'en')]\n","df_2000_en.head()"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["# retrieve groups\n","total_tag_views = df_2000_en.groupby(['tag'])['views'].sum()\n","tag_freq = df_2000_en.groupby(['tag']).size()\n","\n","# create df\n","df_tag = pd.DataFrame({'count' : tag_freq,'total_views' :total_tag_views})\n","df_tag.reset_index(inplace = True, names = 'tag')\n","\n","# print the barplot\n","fig = px.bar(df_tag, x=\"tag\", y=\"count\", color='total_views')\n","fig.show()"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["df_2000_en.sort_values(by = 'views', ascending=False).head(10)"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["results = df_2000_en[\"lyrics\"].apply(clean_text)\n","\n","docs = results.to_list()\n","docs[:2]"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model_2000 = Top2Vec(docs, speed = 'fast-learn', workers = multiprocessing.cpu_count())"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["today = datetime.now().strftime(\"%d%m%Y\")\n","\n","path = \"models/\" + \"top2vec_2000_\" + today\n","\n","model_2000.save(path)\n","model_2000 = Top2Vec.load(path)"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model_2000.get_num_topics()"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["topic_words, word_scores, topic_nums = model_2000.get_topics(4)\n","topic_words[0]"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model_2000.generate_topic_wordcloud(topic_nums[0])"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model_2000.generate_topic_wordcloud(topic_nums[1])"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model_2000.generate_topic_wordcloud(topic_nums[2])"]},{"cell_type":"raw","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"source":["model_2000.generate_topic_wordcloud(topic_nums[3])"]},{"cell_type":"markdown","metadata":{},"source":["## References"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2805070,"sourceId":4840139,"sourceType":"datasetVersion"}],"dockerImageVersionId":30407,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
