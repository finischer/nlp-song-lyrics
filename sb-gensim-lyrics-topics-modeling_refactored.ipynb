{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Steps für Topic Modeling\n","\n","# Step 1: Einlesen der bereinigten Daten\n","# Step 2: Filtern nach Genre 'Rap' und Sprache 'Englisch'\n","# Step 3: Bereinigung der Texte\n","# Step 4: Tokenization und Lemmatizer\n","# Step 5: Stop words\n","# Step 6: Parameter für Modell bestimmen\n","# Step 7: LDA Modell anwenden\n","# Step 8: Topics erkunden\n","# Step 9: Topics visualisieren\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:00:44.199453Z","iopub.status.busy":"2023-05-22T11:00:44.199018Z","iopub.status.idle":"2023-05-22T11:00:44.240434Z","shell.execute_reply":"2023-05-22T11:00:44.23943Z","shell.execute_reply.started":"2023-05-22T11:00:44.199407Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from config import data_input_path, kaggle_output_dir"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_raw = pd.read_csv(data_input_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_lyrics = 100_000\n","\n","df = df_raw.sample(num_lyrics)"]},{"cell_type":"markdown","metadata":{},"source":["Batchs of data are randomly loaded in the memory. The number of batchs loaded depends on the memory capacity of the computer running the script. For the analysis, we will only works on the random samples loaded (All the data in Kaggle).  \n","\n","# Exploring the coarse data\n","\n","Let's visualize and explore the coarse data before a part of deeper analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["For each songs, we've got several informations :\n","- title of the song\n","- the tag (genre)\n","- the artist singer name\n","- the release year\n","- the number of page views\n","- the featuring artists names\n","- the lyrics\n","- the genius identifier\n","- Lyrics language according to [CLD3](https://github.com/google/cld3). Not reliable results are NaN. CLD3 is a neural network model for language indentification.\n","- Lyrics language according to [FastText's langid](https://fasttext.cc/docs/en/language-identification.html). Values with low confidence (<0.5) are NaN. FastText's langid is library developped by Facebook’s AI Research lab for efficient learning of word representations and sentence classification. fastText has also published a fast and accurate tool for text-based language identification capable of recognizing more than 170 languages.\n","- Combines language_cld3 and language_ft. Only has a non NaN entry if they both \"agree\"."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# display the size\n","print('Data frame size (row x columns):', df.size)\n","print('Data rows number: ', len(df))\n","print('Number of unique songs (following genius id): ', len(df.id.unique()))"]},{"cell_type":"markdown","metadata":{},"source":["Genius id seems to be the unique rows identifier.\n","\n","Let's vizualise size of the coarse data over years before preprocessing to compare batch distributions. One things to know before vizualise the data, the pickles are create by chunks reading. "]},{"cell_type":"markdown","metadata":{},"source":["The last diplayed table gives us some information about the data. The csv file seems to be sort by id, so the pickle files are then sort too."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:54.585248Z","iopub.status.busy":"2023-05-22T11:05:54.583899Z","iopub.status.idle":"2023-05-22T11:05:57.674807Z","shell.execute_reply":"2023-05-22T11:05:57.673559Z","shell.execute_reply.started":"2023-05-22T11:05:54.585181Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import plotly.express as px"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get some information about the pickle data\n","def pickle_informations(loader: Loader):\n","    rows = []\n","    for i in range(1, len(os.listdir('data')) + 1):\n","        df = loader.load_pickle(i)\n","        rows.append(len(df))\n","        del df\n","    return rows\n","\n","# get the rows\n","rows = pickle_informations(loader=loader)\n","\n","# create the dataframe\n","df_data = pd.DataFrame(\n","    {'batch' : ['data ' + str(i) for i in range(1,len(rows) + 1)],\n","    'rows' : rows})\n","\n","fig = px.bar(df_data, x=\"batch\", y=\"rows\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["Batch seems to have the same number of rows rexcept for the last one which is consistent because batch are create iteratively by 10e6 chunks over the csv The last batch could be seen as a rest."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from utils.plots import multi_barplot\n","import plotly.colors as col\n","\n","# create the color list\n","colors = col.qualitative.Plotly\n","\n","# 1990 - 2023\n","fig1 = multi_barplot(year1=1960, year2=1989, colors=colors, loader=loader)\n","fig1.show()\n","# 1960 - 1990\n","fig2 = multi_barplot(year1=1990, year2=2023, colors=colors, loader=loader)\n","fig2.show()"]},{"cell_type":"markdown","metadata":{},"source":["The first bar chart (1960 - 1989) shows an increasing numbers of data over years. Moreover batch seems to have quite similar distriutions over years. data_1 and data_2 batch quite outperform the 4 others. data_6 batch is weaker than the other due to its poor number of rows.\n","The data behaves similarly until 2012 as we can see on the second chart (1990-2023). After this year there is great increasing of the data retrieved. A minimum increase of at least 100% of the batch can be observed. An increase of up to 50 times the batch size for some."]},{"cell_type":"markdown","metadata":{},"source":["# Data pre-processing\n","\n","The aim of this part is to preprocess data in order to get suitable data for the analysis. let's focus on the year variable.\n","\n","We will focus on English songs, to facilitate the analysis and the work of natural language processing algorithms."]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:57.687737Z","iopub.status.busy":"2023-05-22T11:05:57.687289Z","iopub.status.idle":"2023-05-22T11:05:58.032745Z","shell.execute_reply":"2023-05-22T11:05:58.031326Z","shell.execute_reply.started":"2023-05-22T11:05:57.687666Z"},"trusted":true},"outputs":[],"source":["# filter by language\n","df = df[df.language == 'en']\n","\n","# filter by genre\n","df = df[df.tag == 'rap']"]},{"cell_type":"markdown","metadata":{},"source":["## Clean lyrics"]},{"cell_type":"code","execution_count":64,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["\"Yo Yo Yo Yo\\n\\nDamn\\n\\n[Verse 1]\\nHeard what they said about me (You heard?)\\nDo you believe what you read about me\\nSay my friends wouldn't be dead without me\\nI heard they're 'posed to be ,real talk, scared about me\\nLike I'm dead one talkin' (He ain't comin' home)\\nEven say I ain't hood no more (Why?)\\nCuz they don't see me often\\nMade fun of my cancer\\nLike I was lyin' bout it\\nWhile my family cryin' bout it\\nSo tell me why tha fuck would I be lyin' bout it (Huh)\\nStupid ass\\nThey threw jabs at my kids on Instagram\\nNow I give a damn\\nHoe you hatin'\\nSend yo location\\nI'll send a killer clan\\nThey say I tried to bribe tha police\\nWhen I had my pistol\\nYou'll rather me talk shit nigga\\nCuz you don't wanna see me whisper\\nShhhhhh\\n[Chorus]\\nPeople talk, but don't know (Repeated)\\n\\n[Verse 2]\\nThey say that I'm crazy\\nThey say I was raised wrong\\nSay my music was weak (What?!)\\nHow can it be weak when it's deep\\nYou just don't know nuthin bout tha streets\\nIn court, I was called an animal\\nThey say I'm a leader of wolves\\nTha police\\nThey lied up in court\\nAll of this shit was coerced (For real!)\\nTha people they talk in tha streets\\nLike they know everybody business\\nYou bunch of bitches\\nBringin' up names they think did it\\nYou act like a whole fuckin' witness\\nTell me this\\nWas you at the crime scene hoe? (Tell me that)\\nDid you see it from tha car?\\nIf you saw it from ya car\\nDid you have on ya high beams hoe?\\nThis how I know\\n[Chorus]\\nPeople talk, but don't know (Repeated)\\n\\n[Verse 3]\\nSticks and stones may break my bones\\nAnd words might even hurt me\\nBut dirt on my name\\nThey searchin'\\nMy haters, they so thirsty\\nThey said that they murked me\\nOn YouTube, they said I died up in a car wreck\\nSaid I had a fake rapper on stage\\nRappin' for me\\nAnd ya'll bought that\\nWhat you call that\\nDeceitful motherfuckers\\nEvil motherfuckers\\nYa'll cowards\\nAny nigga talk down on me\\nYo mouth sour\\nThey even down talk my women\\nCuz them hoes wanna be my women\\nSo they talk, but don't know\\n\\n[Chorus]\\nPeople talk, but don't know (Repeated)\\n[BOOSIE TALKING]\\nMotherfuckers talk about ya like they know ya\\nTalk about ya like a dog\\nThey don't even know what's goin' on\\n\\nTalkin' bout other people kids\\nWhat about yo kids\\nTalkin' bout how people raise they kids\\nWhere yo baby daddy at\\nWhat you got\\nWhat's yo goal in life\\nBut you talk about everybody you see\\n\\nGul look, that hoe sha\\nNo bitch, you sha\\nFor worrying bout that hoe\\nBut I understand what you do though\\n\\nTalk, but don't know\""]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["# get the first lyrics\n","df.iloc[0][\"lyrics\"]"]},{"cell_type":"markdown","metadata":{},"source":["There is many undesirable characters like the line breaker '\\n', figures or square, curly and simple brackets. So let's clean this data with regular expressions."]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:04.712168Z","iopub.status.busy":"2023-05-22T11:09:04.711677Z","iopub.status.idle":"2023-05-22T11:09:04.720611Z","shell.execute_reply":"2023-05-22T11:09:04.71899Z","shell.execute_reply.started":"2023-05-22T11:09:04.712122Z"},"trusted":true},"outputs":[],"source":["import re\n","from numpy.random import randint\n","\n","def clean_text(text):\n","    # remove \\n\n","    text = text.replace('\\n', ' ')\n","    # remove punctuation\n","    text = re.sub(r'[,\\.!?]', '', text)\n","    #removing text in square braquet\n","    text = re.sub(r'\\[.*?\\]', ' ', text)\n","    #removing numbers\n","    text = re.sub(r'\\w*\\d\\w*',' ', text)\n","    #removing bracket\n","    text = re.sub(r'[()]', ' ', text)\n","    # convert all words in lower case\n","    text = text.lower()\n","    return text"]},{"cell_type":"code","execution_count":66,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["\"yo yo yo yo  damn    heard what they said about me  you heard  do you believe what you read about me say my friends wouldn't be dead without me i heard they're 'posed to be real talk scared about me like i'm dead one talkin'  he ain't comin' home  even say i ain't hood no more  why  cuz they don't see me often made fun of my cancer like i was lyin' bout it while my family cryin' bout it so tell me why tha fuck would i be lyin' bout it  huh  stupid ass they threw jabs at my kids on instagram now i give a damn hoe you hatin' send yo location i'll send a killer clan they say i tried to bribe tha police when i had my pistol you'll rather me talk shit nigga cuz you don't wanna see me whisper shhhhhh   people talk but don't know  repeated     they say that i'm crazy they say i was raised wrong say my music was weak  what  how can it be weak when it's deep you just don't know nuthin bout tha streets in court i was called an animal they say i'm a leader of wolves tha police they lied up in court all of this shit was coerced  for real  tha people they talk in tha streets like they know everybody business you bunch of bitches bringin' up names they think did it you act like a whole fuckin' witness tell me this was you at the crime scene hoe  tell me that  did you see it from tha car if you saw it from ya car did you have on ya high beams hoe this how i know   people talk but don't know  repeated     sticks and stones may break my bones and words might even hurt me but dirt on my name they searchin' my haters they so thirsty they said that they murked me on youtube they said i died up in a car wreck said i had a fake rapper on stage rappin' for me and ya'll bought that what you call that deceitful motherfuckers evil motherfuckers ya'll cowards any nigga talk down on me yo mouth sour they even down talk my women cuz them hoes wanna be my women so they talk but don't know    people talk but don't know  repeated    motherfuckers talk about ya like they know ya talk about ya like a dog they don't even know what's goin' on  talkin' bout other people kids what about yo kids talkin' bout how people raise they kids where yo baby daddy at what you got what's yo goal in life but you talk about everybody you see  gul look that hoe sha no bitch you sha for worrying bout that hoe but i understand what you do though  talk but don't know\""]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# get the results of data cleaning\n","cleaned_text = df[\"lyrics\"].apply(clean_text)\n","\n","# convert cleaned text to list\n","docs = cleaned_text.to_list()\n","docs[0]"]},{"cell_type":"code","execution_count":67,"metadata":{"scrolled":true,"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>tag</th>\n","      <th>artist</th>\n","      <th>year</th>\n","      <th>views</th>\n","      <th>features</th>\n","      <th>lyrics</th>\n","      <th>id</th>\n","      <th>language_cld3</th>\n","      <th>language_ft</th>\n","      <th>language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1936379</th>\n","      <td>People Talk But Dont Know</td>\n","      <td>rap</td>\n","      <td>Boosie Badazz</td>\n","      <td>2016</td>\n","      <td>4940</td>\n","      <td>{}</td>\n","      <td>yo yo yo yo  damn    heard what they said abou...</td>\n","      <td>2932434</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>4843522</th>\n","      <td>Going Insane</td>\n","      <td>rap</td>\n","      <td>Genre [PA]</td>\n","      <td>2021</td>\n","      <td>14</td>\n","      <td>{}</td>\n","      <td>and i got no one left to blame except the on...</td>\n","      <td>7378856</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>2329579</th>\n","      <td>Fall</td>\n","      <td>rap</td>\n","      <td>Hipper</td>\n","      <td>2018</td>\n","      <td>161</td>\n","      <td>{}</td>\n","      <td>when i rap on your project i hijack hihats d...</td>\n","      <td>3516737</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             title  tag         artist  year  views features  \\\n","1936379  People Talk But Dont Know  rap  Boosie Badazz  2016   4940       {}   \n","4843522               Going Insane  rap     Genre [PA]  2021     14       {}   \n","2329579                       Fall  rap         Hipper  2018    161       {}   \n","\n","                                                    lyrics       id  \\\n","1936379  yo yo yo yo  damn    heard what they said abou...  2932434   \n","4843522    and i got no one left to blame except the on...  7378856   \n","2329579    when i rap on your project i hijack hihats d...  3516737   \n","\n","        language_cld3 language_ft language  \n","1936379            en          en       en  \n","4843522            en          en       en  \n","2329579            en          en       en  "]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["# update dataframe\n","df.update(cleaned_text)\n","df.head(3)"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.status.busy":"2023-05-22T08:48:48.8135Z","iopub.status.idle":"2023-05-22T08:48:48.814309Z","shell.execute_reply":"2023-05-22T08:48:48.813908Z","shell.execute_reply.started":"2023-05-22T08:48:48.813854Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"yo yo yo yo  damn    heard what they said about me  you heard  do you believe what you read about me say my friends wouldn't be dead without me i heard they're 'posed to be real talk scared about me like i'm dead one talkin'  he ain't comin' home  even say i ain't hood no more  why  cuz they don't see me often made fun of my cancer like i was lyin' bout it while my family cryin' bout it so tell me why tha fuck would i be lyin' bout it  huh  stupid ass they threw jabs at my kids on instagram now i give a damn hoe you hatin' send yo location i'll send a killer clan they say i tried to bribe tha police when i had my pistol you'll rather me talk shit nigga cuz you don't wanna see me whisper shhhhhh   people talk but don't know  repeated     they say that i'm crazy they say i was raised wrong say my music was weak  what  how can it be weak when it's deep you just don't know nuthin bout tha streets in court i was called an animal they say i'm a leader of wolves tha police they lied up in court all of this shit was coerced  for real  tha people they talk in tha streets like they know everybody business you bunch of bitches bringin' up names they think did it you act like a whole fuckin' witness tell me this was you at the crime scene hoe  tell me that  did you see it from tha car if you saw it from ya car did you have on ya high beams hoe this how i know   people talk but don't know  repeated     sticks and stones may break my bones and words might even hurt me but dirt on my name they searchin' my haters they so thirsty they said that they murked me on youtube they said i died up in a car wreck said i had a fake rapper on stage rappin' for me and ya'll bought that what you call that deceitful motherfuckers evil motherfuckers ya'll cowards any nigga talk down on me yo mouth sour they even down talk my women cuz them hoes wanna be my women so they talk but don't know    people talk but don't know  repeated    motherfuckers talk about ya like they know ya talk about ya like a dog they don't even know what's goin' on  talkin' bout other people kids what about yo kids talkin' bout how people raise they kids where yo baby daddy at what you got what's yo goal in life but you talk about everybody you see  gul look that hoe sha no bitch you sha for worrying bout that hoe but i understand what you do though  talk but don't know\""]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["df.iloc[0]['lyrics']"]},{"cell_type":"markdown","metadata":{},"source":["# Topic modeling\n","\n","- [LDA (latent dirichlet allocation)](https://fr.wikipedia.org/wiki/Allocation_de_Dirichlet_latente) are the common way to do topic modeling in the few last years, it works and it's quite easy to use with common python library like [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html).\n","\n","## Define default tokenizer and Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:10:06.836787Z","iopub.status.busy":"2023-05-22T11:10:06.836321Z","iopub.status.idle":"2023-05-22T11:10:06.871445Z","shell.execute_reply":"2023-05-22T11:10:06.870292Z","shell.execute_reply.started":"2023-05-22T11:10:06.836742Z"},"trusted":true},"outputs":[],"source":["from utils.terms_document_matrix import TermsDocumentsMatrix\n","from utils.processing import preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# first decades\n","tdm = TermsDocumentsMatrix(df, decades = [1960, 1970, 1980, 1990],\n","                           colorscale = 'Plasma')\n","\n","# display bar charts of most frequent terms\n","tdm.most_freq_terms(n_rows = 2, n_cols = 2, n_terms = 15)"]},{"cell_type":"markdown","metadata":{},"source":["According to the bar graphs displayed above, a group of words seems to recur on each decade: Love, know, go, feel ... Words that seem to relate to the popular song that can talk about love. This is consistent with our previous analysis from the pie charts showing the proportions of musical styles across time. We also notice an important presence of onomatopoeia like yeah or oh."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# first decades\n","tdm = TermsDocumentsMatrix(df, decades = [2000, 2010, 2020],\n","                          colorscale = 'Plasma')\n","\n","# most frequent terms\n","tdm.most_freq_terms(n_rows = 2, n_cols = 2, n_terms = 15)"]},{"cell_type":"markdown","metadata":{},"source":["We get similar results on this second decade with similar high occurrence words. We see a greater amount of onomatopoeia in the current decade. We can explain this by an emergence of the rap music style on this current and last decade. There is in this style of music a very used process, the 'ad-libs'. They are sounds, words or onomatopoeias that the artists pronounce sometimes between two verses or at the end of a sentence to give more impact to their text and to dynamize the atmosphere of a title. This may explain the greater presence of onomatopoeia in the lyrics of this decade.\n","\n","\n","## Topic modeling with LDA\n","\n","LDA is a common technic use in topic modeling, we firstly process basic preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bigram_measures = nltk.collocations.BigramAssocMeasures()\n","finder = nltk.collocations.BigramCollocationFinder.from_documents([comment.split() for comment in df.lyrics])\n","# Filter only those that occur at least 50 times\n","finder.apply_freq_filter(50)\n","bigram_scores = finder.score_ngrams(bigram_measures.pmi)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trigram_measures = nltk.collocations.TrigramAssocMeasures()\n","finder = nltk.collocations.TrigramCollocationFinder.from_documents([comment.split() for comment in df.lyrics])\n","# Filter only those that occur at least 50 times\n","finder.apply_freq_filter(50)\n","trigram_scores = finder.score_ngrams(trigram_measures.pmi)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trigram_pmi = pd.DataFrame(trigram_scores)\n","trigram_pmi.columns = ['trigram', 'pmi']\n","trigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bigram_pmi = pd.DataFrame(bigram_scores)\n","bigram_pmi.columns = ['bigram', 'pmi']\n","bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Filter for bigrams with only noun-type structures\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","stop_word_list = set(stopwords.words('english'))\n","\n","def bigram_filter(bigram):\n","    tag = nltk.pos_tag(bigram)\n","    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n","        return False\n","    if bigram[0] in stop_word_list or bigram[1] in stop_word_list:\n","        return False\n","    if 'n' in bigram or 't' in bigram:\n","        return False\n","    if 'PRON' in bigram:\n","        return False\n","    return True\n","\n","# Filter for trigrams with only noun-type structures\n","def trigram_filter(trigram):\n","    tag = nltk.pos_tag(trigram)\n","    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n","        return False\n","    if trigram[0] in stop_word_list or trigram[-1] in stop_word_list or trigram[1] in stop_word_list:\n","        return False\n","    if 'n' in trigram or 't' in trigram:\n","         return False\n","    if 'PRON' in trigram:\n","        return False\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('averaged_perceptron_tagger_eng')\n","# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n","# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n","filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n","                                              bigram_filter(bigram['bigram'])\\\n","                                              and bigram.pmi > 5, axis = 1)][:500]\n","\n","filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n","                                                 trigram_filter(trigram['trigram'])\\\n","                                                 and trigram.pmi > 5, axis = 1)][:500]\n","\n","\n","bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n","trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# examples of bigrams\n","bigrams[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# examples of trigrams\n","trigrams[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate n-grams\n","def replace_ngram(x):\n","    for gram in trigrams:\n","        x = x.replace(gram, '_'.join(gram.split()))\n","    for gram in bigrams:\n","        x = x.replace(gram, '_'.join(gram.split()))\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lyrics_w_ngrams = df.copy()\n","lyrics_w_ngrams.reviewText = lyrics_w_ngrams.lyrics.map(lambda x: replace_ngram(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenize reviews + remove stop words + remove names + remove words with less than 2 characters\n","lyrics_w_ngrams = lyrics_w_ngrams.lyrics.map(lambda x: [word for word in x.split()\\\n","                                                 if word not in stop_word_list\\\n","                                                            #   and word not in english_names\\\n","                                                              and len(word) > 2])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lyrics_w_ngrams.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from gensim import corpora\n","from gensim.utils import simple_preprocess\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('stopwords')\n","\n","# Allgemeine Stop-Wörter\n","common_stop_words = {\n","    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \"he\", \"her\",\n","    \"his\", \"I\", \"in\", \"is\", \"it\", \"me\", \"my\", \"of\", \"on\", \"or\", \"she\", \"so\",\n","    \"that\", \"the\", \"to\", \"up\", \"was\", \"we\", \"with\", \"you\", \"i'm\", \"i've\", \"i'll\", \"i'd\", \"i\", \"im\", \"it's\", \"its\", \"don't\", \"dont\", \"i’m\", \"i’ve\", \"i’ll\", \"i’d\", \"i\", \"im\", \"it’s\", \"its\", \"don’t\", \"dont\",\n","}\n","\n","# Spezifische Rap-Stop-Wörter\n","rap_specific_stop_words = {\n","    \"ain't\", \"yeah\", \"yea\", \"yo\", \"uh\", \"huh\", \"gonna\", \"wanna\", \"hey\", \"ooh\", \"woo\",\n","    \"nah\", \"got\", \"gotcha\", \"cuz\", \"y'all\", \"imma\", \"lil\", \"flex\"\n","}\n","\n","# Kombiniertes Set von Stop-Wörtern\n","rap_stop_words = common_stop_words.union(rap_specific_stop_words)\n","\n","# concatenate the stop words\n","stop_words = set(stopwords.words('english')).union(rap_stop_words)\n","\n","# Filter for only nouns\n","def noun_only(x):\n","    pos_comment = nltk.pos_tag(x)\n","    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n","    # to filter both noun and verbs\n","    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n","    return filtered\n","\n","def preprocess_lyrics(lyrics):\n","    return [word for word in lyrics if word not in stop_words]\n","\n","lyrics = lyrics_w_ngrams.map(preprocess_lyrics)\n","final_lyrics = lyrics.map(noun_only)\n","dictionary = corpora.Dictionary(final_lyrics)\n","doc_term_matrix = [dictionary.doc2bow(doc) for doc in final_lyrics]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gensim\n","\n","coherence = []\n","for k in range(5,25):\n","    print('Round: '+str(k))\n","    Lda = gensim.models.ldamodel.LdaModel\n","    ldamodel = Lda(doc_term_matrix, num_topics=k, id2word = dictionary, passes=40,\\\n","                   iterations=200, chunksize = 10000, eval_every = None)\n","    \n","    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=final_lyrics,\\\n","                                                     dictionary=dictionary, coherence='c_v')\n","    coherence.append((k,cm.get_coherence()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_val = [x[0] for x in coherence]\n","y_val = [x[1] for x in coherence]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","plt.plot(x_val,y_val)\n","plt.scatter(x_val,y_val)\n","plt.title('Number of Topics vs. Coherence')\n","plt.xlabel('Number of Topics')\n","plt.ylabel('Coherence')\n","plt.xticks(x_val)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Lda = gensim.models.ldamodel.LdaModel\n","ldamodel = Lda(doc_term_matrix, num_topics=19, id2word = dictionary, passes=40,\\\n","               iterations=200,  chunksize = 10000, eval_every = None, random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ldamodel.show_topics(19, num_words=30, formatted=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pyLDAvis.gensim\n","\n","topic_data =  pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, mds = 'pcoa', R=10, sort_topics=False)\n","pyLDAvis.display(topic_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.utils import simple_preprocess\n","from nltk.corpus import stopwords\n","import pyLDAvis\n","import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","import nltk\n","\n","# NLTK-Stopwörter herunterladen\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","# Datenvorverarbeitung und Tokenisierung\n","def preprocess_lyrics(lyrics):\n","    return [word for word in simple_preprocess(lyrics) if word not in stop_words]\n","\n","\n","df['tokens'] = df['lyrics'].apply(preprocess_lyrics)\n","\n","\n","# Erstelle Wörterbuch und Korpus\n","dictionary = corpora.Dictionary(df['tokens'])\n","corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n","\n","# LDA-Modellierung\n","num_topics = 23\n","lda_model = LdaModel(\n","    corpus=corpus, \n","    id2word=dictionary, \n","    num_topics=num_topics, \n","    random_state=42, \n","    update_every=1, \n","    chunksize=10,\n","    passes=20, \n","    alpha='auto', \n","    per_word_topics=True\n",")\n","\n","# Themen anzeigen\n","for idx, topic in lda_model.print_topics(num_words=15):\n","    print(f\"Topic {idx}: {topic}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualisierung\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds='mmds', R=15)\n","pyLDAvis.show(vis, local=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Topic Modeling LDA v2"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["from nltk.tokenize import RegexpTokenizer\n","\n","lyric_corpus_tokenized = []\n","tokenizer = RegexpTokenizer(r'\\w+')\n","for lyric in df.lyrics:\n","    tokenized_lyric = tokenizer.tokenize(lyric.lower())\n","    lyric_corpus_tokenized.append(tokenized_lyric)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# filter all tokens out that are less than 3 characters long\n","for s,song in enumerate(lyric_corpus_tokenized):\n","    filtered_song = []    \n","    for token in song:\n","        if len(token) > 2 and not token.isnumeric():\n","            filtered_song.append(token)\n","    lyric_corpus_tokenized[s] = filtered_song"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/niklasfischer/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","for s,song in enumerate(lyric_corpus_tokenized):\n","    lemmatized_tokens = []\n","    for token in song:\n","        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n","    lyric_corpus_tokenized[s] = lemmatized_tokens"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","new_stop_words = ['ooh','yeah','hey','whoa','woah', 'ohh', 'was', 'mmm', 'oooh','yah','yeh','mmm', 'hmm','deh','doh','jah','wa']\n","stop_words.extend(new_stop_words)\n","for s,song in enumerate(lyric_corpus_tokenized):\n","    filtered_text = []    \n","    for token in song:\n","        if token not in stop_words:\n","            filtered_text.append(token)\n","    lyric_corpus_tokenized[s] = filtered_text"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["from gensim.corpora import Dictionary\n","dictionary = Dictionary(lyric_corpus_tokenized)\n","dictionary.filter_extremes(no_below = 100, no_above = 0.8)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["from gensim.corpora import MmCorpus\n","gensim_corpus = [dictionary.doc2bow(song) for song in lyric_corpus_tokenized]\n","temp = dictionary[0]\n","id2word = dictionary.id2token"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["chunksize = 2000\n","passes = 20\n","iterations = 400\n","num_topics = 10"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["from gensim.models import LdaModel\n","lda_model = LdaModel(\n","    corpus=gensim_corpus,\n","    id2word=id2word,\n","    chunksize=chunksize,\n","    alpha='auto',\n","    eta='auto',\n","    iterations=iterations,\n","    num_topics=num_topics,\n","    passes=passes\n",")"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.34659874777052263\n"]}],"source":["from gensim.models.coherencemodel import CoherenceModel\n","coherencemodel = CoherenceModel(model=lda_model, texts=lyric_corpus_tokenized, dictionary=dictionary, coherence='c_v')\n","print(coherencemodel.get_coherence())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n","#pyLDAvis.display(vis_data)\n","pyLDAvis.save_html(vis_data, f'./Lyrics_LDA_k_{str(num_topics)}_n_{str(num_lyrics)}.html')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2805070,"sourceId":4840139,"sourceType":"datasetVersion"}],"dockerImageVersionId":30407,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
