{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Steps für Topic Modeling\n","\n","# Step 1: Einlesen der bereinigten Daten\n","# Step 2: Filtern nach Genre 'Rap' und Sprache 'Englisch'\n","# Step 3: Bereinigung der Texte\n","# Step 4: Tokenization und Lemmatizer\n","# Step 5: Stop words\n","# Step 6: Parameter für Modell bestimmen\n","# Step 7: LDA Modell anwenden\n","# Step 8: Topics erkunden\n","# Step 9: Topics visualisieren\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:00:44.199453Z","iopub.status.busy":"2023-05-22T11:00:44.199018Z","iopub.status.idle":"2023-05-22T11:00:44.240434Z","shell.execute_reply":"2023-05-22T11:00:44.23943Z","shell.execute_reply.started":"2023-05-22T11:00:44.199407Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from config import data_input_path, kaggle_output_dir"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["df_raw = pd.read_csv(data_input_path)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["num_lyrics = 100_000\n","\n","# df = df_raw.sample(num_lyrics)\n","df = df_raw"]},{"cell_type":"markdown","metadata":{},"source":["Batchs of data are randomly loaded in the memory. The number of batchs loaded depends on the memory capacity of the computer running the script. For the analysis, we will only works on the random samples loaded (All the data in Kaggle).  \n","\n","# Exploring the coarse data\n","\n","Let's visualize and explore the coarse data before a part of deeper analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["For each songs, we've got several informations :\n","- title of the song\n","- the tag (genre)\n","- the artist singer name\n","- the release year\n","- the number of page views\n","- the featuring artists names\n","- the lyrics\n","- the genius identifier\n","- Lyrics language according to [CLD3](https://github.com/google/cld3). Not reliable results are NaN. CLD3 is a neural network model for language indentification.\n","- Lyrics language according to [FastText's langid](https://fasttext.cc/docs/en/language-identification.html). Values with low confidence (<0.5) are NaN. FastText's langid is library developped by Facebook’s AI Research lab for efficient learning of word representations and sentence classification. fastText has also published a fast and accurate tool for text-based language identification capable of recognizing more than 170 languages.\n","- Combines language_cld3 and language_ft. Only has a non NaN entry if they both \"agree\"."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Data frame size (row x columns): 56483416\n","Data rows number:  5134856\n","Number of unique songs (following genius id):  5134856\n"]}],"source":["# display the size\n","print('Data frame size (row x columns):', df.size)\n","print('Data rows number: ', len(df))\n","print('Number of unique songs (following genius id): ', len(df.id.unique()))"]},{"cell_type":"markdown","metadata":{},"source":["Genius id seems to be the unique rows identifier.\n","\n","Let's vizualise size of the coarse data over years before preprocessing to compare batch distributions. One things to know before vizualise the data, the pickles are create by chunks reading. "]},{"cell_type":"markdown","metadata":{},"source":["The last diplayed table gives us some information about the data. The csv file seems to be sort by id, so the pickle files are then sort too."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:54.585248Z","iopub.status.busy":"2023-05-22T11:05:54.583899Z","iopub.status.idle":"2023-05-22T11:05:57.674807Z","shell.execute_reply":"2023-05-22T11:05:57.673559Z","shell.execute_reply.started":"2023-05-22T11:05:54.585181Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import plotly.express as px"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# get some information about the pickle data\n","def pickle_informations(loader: Loader):\n","    rows = []\n","    for i in range(1, len(os.listdir('data')) + 1):\n","        df = loader.load_pickle(i)\n","        rows.append(len(df))\n","        del df\n","    return rows\n","\n","# get the rows\n","rows = pickle_informations(loader=loader)\n","\n","# create the dataframe\n","df_data = pd.DataFrame(\n","    {'batch' : ['data ' + str(i) for i in range(1,len(rows) + 1)],\n","    'rows' : rows})\n","\n","fig = px.bar(df_data, x=\"batch\", y=\"rows\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["Batch seems to have the same number of rows rexcept for the last one which is consistent because batch are create iteratively by 10e6 chunks over the csv The last batch could be seen as a rest."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from utils.plots import multi_barplot\n","import plotly.colors as col\n","\n","# create the color list\n","colors = col.qualitative.Plotly\n","\n","# 1990 - 2023\n","fig1 = multi_barplot(year1=1960, year2=1989, colors=colors, loader=loader)\n","fig1.show()\n","# 1960 - 1990\n","fig2 = multi_barplot(year1=1990, year2=2023, colors=colors, loader=loader)\n","fig2.show()"]},{"cell_type":"markdown","metadata":{},"source":["The first bar chart (1960 - 1989) shows an increasing numbers of data over years. Moreover batch seems to have quite similar distriutions over years. data_1 and data_2 batch quite outperform the 4 others. data_6 batch is weaker than the other due to its poor number of rows.\n","The data behaves similarly until 2012 as we can see on the second chart (1990-2023). After this year there is great increasing of the data retrieved. A minimum increase of at least 100% of the batch can be observed. An increase of up to 50 times the batch size for some."]},{"cell_type":"markdown","metadata":{},"source":["# Data pre-processing\n","\n","The aim of this part is to preprocess data in order to get suitable data for the analysis. let's focus on the year variable.\n","\n","We will focus on English songs, to facilitate the analysis and the work of natural language processing algorithms."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:05:57.687737Z","iopub.status.busy":"2023-05-22T11:05:57.687289Z","iopub.status.idle":"2023-05-22T11:05:58.032745Z","shell.execute_reply":"2023-05-22T11:05:58.031326Z","shell.execute_reply.started":"2023-05-22T11:05:57.687666Z"},"trusted":true},"outputs":[],"source":["# filter by language\n","df = df[df.language == 'en']\n","\n","# filter by genre \n","df = df[df.tag == 'rap']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num rows: 964605\n"]}],"source":["print(f\"Num rows: {len(df)}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["start_year = 1960\n","end_year = 2024\n","\n","# filter by year\n","df = df[(df.year >= start_year) & (df.year <= end_year)]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num rows: 963624\n"]}],"source":["print(f\"Num rows: {len(df)}\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# drop nan values\n","df = df.dropna(subset=['lyrics'])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num rows: 963624\n"]}],"source":["print(f\"Num rows: {len(df)}\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# remove artitst starts with 'Genius'\n","df = df[~df.artist.str.startswith('Genius')]"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num rows: 959134\n"]}],"source":["print(f\"Num rows: {len(df)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Clean lyrics"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'[Chorus: Opera Steve & Cam\\'ron]\\nKilla Cam, Killa Cam, Cam\\nKilla Cam, Killa Cam\\nKilla Cam, Cam\\nKilla Cam, Killa Cam, Cam\\nKilla Killa Killa Cam\\nKilla Cam, Cam, Killa (Killa!)\\nKilla Cam, Killa Cam, Cam (Bases loaded)\\nKilla Cam, Killa Cam (Uh-huh)\\nKilla Cam, Cam (Santana on second, Jim on third)\\nKilla Cam, Killa Cam, Cam (I\\'m at bat)\\nKilla Killa Killa Cam\\nKilla Cam, Cam, Killa (I\\'m \\'bout to hit this shit out the world)\\nKilla Cam (Ugh, Heatmakerz), Killa Cam, Cam\\nKilla Cam, Killa Cam\\nKilla Cam, Cam (Hahahaha)\\nKilla Cam, Killa Cam, Cam\\nKilla Killa Killa Cam\\nKilla Cam, Cam, Killa (We  make this shit clap)\\nKilla Cam, Killa Cam, Cam\\nKilla Cam, Killa Cam\\nKilla Cam, Cam\\nKilla Cam, Killa Cam, Cam\\nKilla Killa Killa Cam (Killa! Killa!)\\nKilla Cam, Cam, Killa\\n[Verse 1]\\nWith the goons I spar, stay in tune with ma (What up?)\\nShe like, \"Damn, this the realest since \\'Kumbaya\\'\"\\nBomaye, Killa Cam, my Lord (My Lord)\\nStill the man with the pan, scrilla, fam, on board\\nNow bitches, they want to neuter me, niggas, they want to tutor me\\nThe hooligan in Houlihan\\'s, maneuvering\\'s nothing new to me\\nDoggy, I\\'m from the land of grind, pan-pan: gram or dime?\\nNot toes or MC when I say \"hammer time\"\\nBeef: I hammer mine, when I get my hands on nines\\nIf I had on \\'Bama line, Corduroys, Cam\\'ll shine\\nCanary burgundy: I call it \"Lemon Red\" (Red)\\nYellow diamonds in my ear, call \\'em \"Lemonheads\"\\nLemonhead, end up dead, ice like Winnipeg\\nGemstone, Flintstones, you could say I\\'m friends with Fred\\nYou unhappy, scrappy? (What\\'s going on, Scrappy?)\\nI got Pataki at me\\nBitches say I\\'m \"Tacky Daddy,\" Range look like Laffy Taffy\\n\\n[Chorus]\\nKilla Cam\\nKilla Cam Cam (sing)\\nKilla Cam Killa Cam\\nKilla Cam Cam (uhh, it\\'s me, clap)\\nKilla Cam\\nKilla Cam Cam\\nKilla Killa Killa Cam (sing)\\nKilla Cam Cam Killa (uhh, it\\'s me, clap)\\nKilla Cam\\nKilla Cam Cam (sing)\\nKilla Cam Killa Cam\\nKilla Cam Cam (clap, it\\'s me)\\nKilla Cam\\nKilla Cam Cam\\nKilla Killa Killa Cam (clap)\\n(Harlem, I know y\\'all know about this)\\nKilla Cam Cam Killa (Killa!)\\n[Verse 2]\\nYo, I\\'m from where Nicky Barnes got rich as fuck\\nRich and A hit the kitchens then were pitchin\\' up\\nRob Base, Mase, Doug E Fresh switched it up\\nI do both, who am I to fuck tradition up? (Killa!)\\nSo I parked in a tow-away zone\\nChrome...I don\\'t care\\nThat car a throwaway, homes (Killa!)\\nWelcome to Harlem, where you welcome to problems\\nOff of furlough, fellow felons get pardons\\nThem niggas knew we bang\\nStood out like Pootie Tang\\nSoon as the stoolie sings\\nThat when the toolie sing!\\nBang! Bang!\\nCame from that movie ring\\nSnap, crack jewelry bling\\nFlapjack, ooh he bring\\nClack-clack, \"ooh he ring!\"\\nBad rap, cuties cling\\nAss cap, put them in the river\\nI\\'m the sushi king\\nAnd I\\'ma keep ya fresh\\nLet the fish eat ya flesh\\nYes sir, please confess\\nJust say he\\'s the best (Killa!)\\n[Chorus]\\nKilla Cam (sing)\\nKilla Cam Cam (clap)\\nKilla Cam Killa Cam (yes)\\nKilla Cam Cam (it\\'s me, sing)\\nKilla Cam\\nKilla Cam Cam (sing)\\nKilla Killa Killa Cam\\nKilla Cam Cam Killa  (clap, yes sir, uhh)\\nKilla Cam\\nKilla Cam Cam (sing, clap)\\nKilla Cam Killa Cam\\nKilla Cam Cam (it\\'s me)\\nKilla Cam (sing, clap)\\nKilla Cam Cam\\nKilla Killa Killa Cam\\n(Let me end this shit, listen)\\nKilla Cam Cam Killa\\n\\n[Verse 3]\\n(Killa!) Yo\\nHow dope is this?\\nTeach you how to rope a chick\\nWhat you want: coke or piff?\\nGot it all, smoke or sniff? (everything)\\nAnd you know my drift\\nUsed to figures, dough and shit (millions)\\nYou a rooster nigga, just a roaster, bitch\\nAnd I roast ya bitch\\nThat\\'s how it usually ends\\nTell her and her groupie friends\\nGo get their coochie cleansed\\nWe the moody Gucci, Louis and Pucci men\\nEscada, Prada\\nThe chopper it got the Uzi lens\\nBird\\'s-eye view\\nThe birds I knew flip birds\\nBird gangs, it was birds I flew\\nAnd word I blew off herb I grew\\nI would serve on stoops\\nNow swerve in coupes\\nIt\\'s me, sing! Killa, uhh\\n\\n[Chorus]\\nKilla Cam\\nKilla Cam Cam\\nKilla Cam Killa Cam\\nKilla Cam Cam\\nKilla Cam\\nKilla Cam Cam\\nKilla Killa Killa Cam\\nKilla Cam Cam Killa\\nKilla Cam\\nKilla Cam Cam\\nKilla Cam Killa Cam\\nKilla Cam Cam\\nKilla Cam\\nKilla Cam Cam\\nKilla Killa Killa Cam\\nKilla Cam Cam Killa'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# get the first lyrics\n","df.iloc[0][\"lyrics\"]"]},{"cell_type":"markdown","metadata":{},"source":["There is many undesirable characters like the line breaker '\\n', figures or square, curly and simple brackets. So let's clean this data with regular expressions."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:09:04.712168Z","iopub.status.busy":"2023-05-22T11:09:04.711677Z","iopub.status.idle":"2023-05-22T11:09:04.720611Z","shell.execute_reply":"2023-05-22T11:09:04.71899Z","shell.execute_reply.started":"2023-05-22T11:09:04.712122Z"},"trusted":true},"outputs":[],"source":["import re\n","from numpy.random import randint\n","\n","def clean_text(text):\n","    # remove \\n\n","    text = text.replace('\\n', ' ')\n","    # remove punctuation\n","    text = re.sub(r'[,\\.!?]', '', text)\n","    #removing text in square braquet\n","    text = re.sub(r'\\[.*?\\]', ' ', text)\n","    #removing numbers\n","    text = re.sub(r'\\w*\\d\\w*',' ', text)\n","    #removing bracket\n","    text = re.sub(r'[()]', ' ', text)\n","    # convert all words in lower case\n","    text = text.lower()\n","    return text"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'  killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam killa cam cam killa  killa  killa cam killa cam cam  bases loaded  killa cam killa cam  uh-huh  killa cam cam  santana on second jim on third  killa cam killa cam cam  i\\'m at bat  killa killa killa cam killa cam cam killa  i\\'m \\'bout to hit this shit out the world  killa cam  ugh heatmakerz  killa cam cam killa cam killa cam killa cam cam  hahahaha  killa cam killa cam cam killa killa killa cam killa cam cam killa  we  make this shit clap  killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam  killa killa  killa cam cam killa   with the goons i spar stay in tune with ma  what up  she like \"damn this the realest since \\'kumbaya\\'\" bomaye killa cam my lord  my lord  still the man with the pan scrilla fam on board now bitches they want to neuter me niggas they want to tutor me the hooligan in houlihan\\'s maneuvering\\'s nothing new to me doggy i\\'m from the land of grind pan-pan: gram or dime not toes or mc when i say \"hammer time\" beef: i hammer mine when i get my hands on nines if i had on \\'bama line corduroys cam\\'ll shine canary burgundy: i call it \"lemon red\"  red  yellow diamonds in my ear call \\'em \"lemonheads\" lemonhead end up dead ice like winnipeg gemstone flintstones you could say i\\'m friends with fred you unhappy scrappy  what\\'s going on scrappy  i got pataki at me bitches say i\\'m \"tacky daddy\" range look like laffy taffy    killa cam killa cam cam  sing  killa cam killa cam killa cam cam  uhh it\\'s me clap  killa cam killa cam cam killa killa killa cam  sing  killa cam cam killa  uhh it\\'s me clap  killa cam killa cam cam  sing  killa cam killa cam killa cam cam  clap it\\'s me  killa cam killa cam cam killa killa killa cam  clap   harlem i know y\\'all know about this  killa cam cam killa  killa    yo i\\'m from where nicky barnes got rich as fuck rich and a hit the kitchens then were pitchin\\' up rob base mase doug e fresh switched it up i do both who am i to fuck tradition up  killa  so i parked in a tow-away zone chromei don\\'t care that car a throwaway homes  killa  welcome to harlem where you welcome to problems off of furlough fellow felons get pardons them niggas knew we bang stood out like pootie tang soon as the stoolie sings that when the toolie sing bang bang came from that movie ring snap crack jewelry bling flapjack ooh he bring clack-clack \"ooh he ring\" bad rap cuties cling ass cap put them in the river i\\'m the sushi king and i\\'ma keep ya fresh let the fish eat ya flesh yes sir please confess just say he\\'s the best  killa    killa cam  sing  killa cam cam  clap  killa cam killa cam  yes  killa cam cam  it\\'s me sing  killa cam killa cam cam  sing  killa killa killa cam killa cam cam killa   clap yes sir uhh  killa cam killa cam cam  sing clap  killa cam killa cam killa cam cam  it\\'s me  killa cam  sing clap  killa cam cam killa killa killa cam  let me end this shit listen  killa cam cam killa     killa  yo how dope is this teach you how to rope a chick what you want: coke or piff got it all smoke or sniff  everything  and you know my drift used to figures dough and shit  millions  you a rooster nigga just a roaster bitch and i roast ya bitch that\\'s how it usually ends tell her and her groupie friends go get their coochie cleansed we the moody gucci louis and pucci men escada prada the chopper it got the uzi lens bird\\'s-eye view the birds i knew flip birds bird gangs it was birds i flew and word i blew off herb i grew i would serve on stoops now swerve in coupes it\\'s me sing killa uhh    killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam killa cam cam killa killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam killa cam cam killa'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# get the results of data cleaning\n","cleaned_text = df[\"lyrics\"].apply(clean_text)\n","\n","# convert cleaned text to list\n","docs = cleaned_text.to_list()\n","docs[0]"]},{"cell_type":"code","execution_count":19,"metadata":{"scrolled":true,"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>tag</th>\n","      <th>artist</th>\n","      <th>year</th>\n","      <th>views</th>\n","      <th>features</th>\n","      <th>lyrics</th>\n","      <th>id</th>\n","      <th>language_cld3</th>\n","      <th>language_ft</th>\n","      <th>language</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Killa Cam</td>\n","      <td>rap</td>\n","      <td>Cam'ron</td>\n","      <td>2004</td>\n","      <td>173166</td>\n","      <td>{\"Cam\\\\'ron\",\"Opera Steve\"}</td>\n","      <td>killa cam killa cam cam killa cam killa cam ...</td>\n","      <td>1</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Can I Live</td>\n","      <td>rap</td>\n","      <td>JAY-Z</td>\n","      <td>1996</td>\n","      <td>468624</td>\n","      <td>{}</td>\n","      <td>yeah hah yeah roc-a-fella we invite you t...</td>\n","      <td>3</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Forgive Me Father</td>\n","      <td>rap</td>\n","      <td>Fabolous</td>\n","      <td>2003</td>\n","      <td>4743</td>\n","      <td>{}</td>\n","      <td>maybe cause i'm eatin and these bastards fiend...</td>\n","      <td>4</td>\n","      <td>en</td>\n","      <td>en</td>\n","      <td>en</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               title  tag    artist  year   views  \\\n","0          Killa Cam  rap   Cam'ron  2004  173166   \n","1         Can I Live  rap     JAY-Z  1996  468624   \n","2  Forgive Me Father  rap  Fabolous  2003    4743   \n","\n","                      features  \\\n","0  {\"Cam\\\\'ron\",\"Opera Steve\"}   \n","1                           {}   \n","2                           {}   \n","\n","                                              lyrics  id language_cld3  \\\n","0    killa cam killa cam cam killa cam killa cam ...   1            en   \n","1       yeah hah yeah roc-a-fella we invite you t...   3            en   \n","2  maybe cause i'm eatin and these bastards fiend...   4            en   \n","\n","  language_ft language  \n","0          en       en  \n","1          en       en  \n","2          en       en  "]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# update dataframe\n","df.update(cleaned_text)\n","df.head(3)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.status.busy":"2023-05-22T08:48:48.8135Z","iopub.status.idle":"2023-05-22T08:48:48.814309Z","shell.execute_reply":"2023-05-22T08:48:48.813908Z","shell.execute_reply.started":"2023-05-22T08:48:48.813854Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'  killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam killa cam cam killa  killa  killa cam killa cam cam  bases loaded  killa cam killa cam  uh-huh  killa cam cam  santana on second jim on third  killa cam killa cam cam  i\\'m at bat  killa killa killa cam killa cam cam killa  i\\'m \\'bout to hit this shit out the world  killa cam  ugh heatmakerz  killa cam cam killa cam killa cam killa cam cam  hahahaha  killa cam killa cam cam killa killa killa cam killa cam cam killa  we  make this shit clap  killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam  killa killa  killa cam cam killa   with the goons i spar stay in tune with ma  what up  she like \"damn this the realest since \\'kumbaya\\'\" bomaye killa cam my lord  my lord  still the man with the pan scrilla fam on board now bitches they want to neuter me niggas they want to tutor me the hooligan in houlihan\\'s maneuvering\\'s nothing new to me doggy i\\'m from the land of grind pan-pan: gram or dime not toes or mc when i say \"hammer time\" beef: i hammer mine when i get my hands on nines if i had on \\'bama line corduroys cam\\'ll shine canary burgundy: i call it \"lemon red\"  red  yellow diamonds in my ear call \\'em \"lemonheads\" lemonhead end up dead ice like winnipeg gemstone flintstones you could say i\\'m friends with fred you unhappy scrappy  what\\'s going on scrappy  i got pataki at me bitches say i\\'m \"tacky daddy\" range look like laffy taffy    killa cam killa cam cam  sing  killa cam killa cam killa cam cam  uhh it\\'s me clap  killa cam killa cam cam killa killa killa cam  sing  killa cam cam killa  uhh it\\'s me clap  killa cam killa cam cam  sing  killa cam killa cam killa cam cam  clap it\\'s me  killa cam killa cam cam killa killa killa cam  clap   harlem i know y\\'all know about this  killa cam cam killa  killa    yo i\\'m from where nicky barnes got rich as fuck rich and a hit the kitchens then were pitchin\\' up rob base mase doug e fresh switched it up i do both who am i to fuck tradition up  killa  so i parked in a tow-away zone chromei don\\'t care that car a throwaway homes  killa  welcome to harlem where you welcome to problems off of furlough fellow felons get pardons them niggas knew we bang stood out like pootie tang soon as the stoolie sings that when the toolie sing bang bang came from that movie ring snap crack jewelry bling flapjack ooh he bring clack-clack \"ooh he ring\" bad rap cuties cling ass cap put them in the river i\\'m the sushi king and i\\'ma keep ya fresh let the fish eat ya flesh yes sir please confess just say he\\'s the best  killa    killa cam  sing  killa cam cam  clap  killa cam killa cam  yes  killa cam cam  it\\'s me sing  killa cam killa cam cam  sing  killa killa killa cam killa cam cam killa   clap yes sir uhh  killa cam killa cam cam  sing clap  killa cam killa cam killa cam cam  it\\'s me  killa cam  sing clap  killa cam cam killa killa killa cam  let me end this shit listen  killa cam cam killa     killa  yo how dope is this teach you how to rope a chick what you want: coke or piff got it all smoke or sniff  everything  and you know my drift used to figures dough and shit  millions  you a rooster nigga just a roaster bitch and i roast ya bitch that\\'s how it usually ends tell her and her groupie friends go get their coochie cleansed we the moody gucci louis and pucci men escada prada the chopper it got the uzi lens bird\\'s-eye view the birds i knew flip birds bird gangs it was birds i flew and word i blew off herb i grew i would serve on stoops now swerve in coupes it\\'s me sing killa uhh    killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam killa cam cam killa killa cam killa cam cam killa cam killa cam killa cam cam killa cam killa cam cam killa killa killa cam killa cam cam killa'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df.iloc[0]['lyrics']"]},{"cell_type":"markdown","metadata":{},"source":["# Topic modeling\n","\n","- [LDA (latent dirichlet allocation)](https://fr.wikipedia.org/wiki/Allocation_de_Dirichlet_latente) are the common way to do topic modeling in the few last years, it works and it's quite easy to use with common python library like [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html).\n","\n","## Define default tokenizer and Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-22T11:10:06.836787Z","iopub.status.busy":"2023-05-22T11:10:06.836321Z","iopub.status.idle":"2023-05-22T11:10:06.871445Z","shell.execute_reply":"2023-05-22T11:10:06.870292Z","shell.execute_reply.started":"2023-05-22T11:10:06.836742Z"},"trusted":true},"outputs":[],"source":["from utils.terms_document_matrix import TermsDocumentsMatrix\n","from utils.processing import preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# first decades\n","tdm = TermsDocumentsMatrix(df, decades = [1960, 1970, 1980, 1990],\n","                           colorscale = 'Plasma')\n","\n","# display bar charts of most frequent terms\n","tdm.most_freq_terms(n_rows = 2, n_cols = 2, n_terms = 15)"]},{"cell_type":"markdown","metadata":{},"source":["According to the bar graphs displayed above, a group of words seems to recur on each decade: Love, know, go, feel ... Words that seem to relate to the popular song that can talk about love. This is consistent with our previous analysis from the pie charts showing the proportions of musical styles across time. We also notice an important presence of onomatopoeia like yeah or oh."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# first decades\n","tdm = TermsDocumentsMatrix(df, decades = [2000, 2010, 2020],\n","                          colorscale = 'Plasma')\n","\n","# most frequent terms\n","tdm.most_freq_terms(n_rows = 2, n_cols = 2, n_terms = 15)"]},{"cell_type":"markdown","metadata":{},"source":["We get similar results on this second decade with similar high occurrence words. We see a greater amount of onomatopoeia in the current decade. We can explain this by an emergence of the rap music style on this current and last decade. There is in this style of music a very used process, the 'ad-libs'. They are sounds, words or onomatopoeias that the artists pronounce sometimes between two verses or at the end of a sentence to give more impact to their text and to dynamize the atmosphere of a title. This may explain the greater presence of onomatopoeia in the lyrics of this decade.\n","\n","\n","## Topic modeling with LDA\n","\n","LDA is a common technic use in topic modeling, we firstly process basic preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bigram_measures = nltk.collocations.BigramAssocMeasures()\n","finder = nltk.collocations.BigramCollocationFinder.from_documents([comment.split() for comment in df.lyrics])\n","# Filter only those that occur at least 50 times\n","finder.apply_freq_filter(50)\n","bigram_scores = finder.score_ngrams(bigram_measures.pmi)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trigram_measures = nltk.collocations.TrigramAssocMeasures()\n","finder = nltk.collocations.TrigramCollocationFinder.from_documents([comment.split() for comment in df.lyrics])\n","# Filter only those that occur at least 50 times\n","finder.apply_freq_filter(50)\n","trigram_scores = finder.score_ngrams(trigram_measures.pmi)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trigram_pmi = pd.DataFrame(trigram_scores)\n","trigram_pmi.columns = ['trigram', 'pmi']\n","trigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bigram_pmi = pd.DataFrame(bigram_scores)\n","bigram_pmi.columns = ['bigram', 'pmi']\n","bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Filter for bigrams with only noun-type structures\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords')\n","stop_word_list = set(stopwords.words('english'))\n","\n","def bigram_filter(bigram):\n","    tag = nltk.pos_tag(bigram)\n","    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n","        return False\n","    if bigram[0] in stop_word_list or bigram[1] in stop_word_list:\n","        return False\n","    if 'n' in bigram or 't' in bigram:\n","        return False\n","    if 'PRON' in bigram:\n","        return False\n","    return True\n","\n","# Filter for trigrams with only noun-type structures\n","def trigram_filter(trigram):\n","    tag = nltk.pos_tag(trigram)\n","    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n","        return False\n","    if trigram[0] in stop_word_list or trigram[-1] in stop_word_list or trigram[1] in stop_word_list:\n","        return False\n","    if 'n' in trigram or 't' in trigram:\n","         return False\n","    if 'PRON' in trigram:\n","        return False\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('averaged_perceptron_tagger_eng')\n","# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n","# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n","filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n","                                              bigram_filter(bigram['bigram'])\\\n","                                              and bigram.pmi > 5, axis = 1)][:500]\n","\n","filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n","                                                 trigram_filter(trigram['trigram'])\\\n","                                                 and trigram.pmi > 5, axis = 1)][:500]\n","\n","\n","bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n","trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# examples of bigrams\n","bigrams[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# examples of trigrams\n","trigrams[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Concatenate n-grams\n","def replace_ngram(x):\n","    for gram in trigrams:\n","        x = x.replace(gram, '_'.join(gram.split()))\n","    for gram in bigrams:\n","        x = x.replace(gram, '_'.join(gram.split()))\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lyrics_w_ngrams = df.copy()\n","lyrics_w_ngrams.reviewText = lyrics_w_ngrams.lyrics.map(lambda x: replace_ngram(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenize reviews + remove stop words + remove names + remove words with less than 2 characters\n","lyrics_w_ngrams = lyrics_w_ngrams.lyrics.map(lambda x: [word for word in x.split()\\\n","                                                 if word not in stop_word_list\\\n","                                                            #   and word not in english_names\\\n","                                                              and len(word) > 2])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lyrics_w_ngrams.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from gensim import corpora\n","from gensim.utils import simple_preprocess\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('stopwords')\n","\n","# Allgemeine Stop-Wörter\n","common_stop_words = {\n","    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \"he\", \"her\",\n","    \"his\", \"I\", \"in\", \"is\", \"it\", \"me\", \"my\", \"of\", \"on\", \"or\", \"she\", \"so\",\n","    \"that\", \"the\", \"to\", \"up\", \"was\", \"we\", \"with\", \"you\", \"i'm\", \"i've\", \"i'll\", \"i'd\", \"i\", \"im\", \"it's\", \"its\", \"don't\", \"dont\", \"i’m\", \"i’ve\", \"i’ll\", \"i’d\", \"i\", \"im\", \"it’s\", \"its\", \"don’t\", \"dont\",\n","}\n","\n","# Spezifische Rap-Stop-Wörter\n","rap_specific_stop_words = {\n","    \"ain't\", \"yeah\", \"yea\", \"yo\", \"uh\", \"huh\", \"gonna\", \"wanna\", \"hey\", \"ooh\", \"woo\",\n","    \"nah\", \"got\", \"gotcha\", \"cuz\", \"y'all\", \"imma\", \"lil\", \"flex\"\n","}\n","\n","# Kombiniertes Set von Stop-Wörtern\n","rap_stop_words = common_stop_words.union(rap_specific_stop_words)\n","\n","# concatenate the stop words\n","stop_words = set(stopwords.words('english')).union(rap_stop_words)\n","\n","# Filter for only nouns\n","def noun_only(x):\n","    pos_comment = nltk.pos_tag(x)\n","    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n","    # to filter both noun and verbs\n","    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n","    return filtered\n","\n","def preprocess_lyrics(lyrics):\n","    return [word for word in lyrics if word not in stop_words]\n","\n","lyrics = lyrics_w_ngrams.map(preprocess_lyrics)\n","final_lyrics = lyrics.map(noun_only)\n","dictionary = corpora.Dictionary(final_lyrics)\n","doc_term_matrix = [dictionary.doc2bow(doc) for doc in final_lyrics]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gensim\n","\n","coherence = []\n","for k in range(5,25):\n","    print('Round: '+str(k))\n","    Lda = gensim.models.ldamodel.LdaModel\n","    ldamodel = Lda(doc_term_matrix, num_topics=k, id2word = dictionary, passes=40,\\\n","                   iterations=200, chunksize = 10000, eval_every = None)\n","    \n","    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=final_lyrics,\\\n","                                                     dictionary=dictionary, coherence='c_v')\n","    coherence.append((k,cm.get_coherence()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_val = [x[0] for x in coherence]\n","y_val = [x[1] for x in coherence]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","plt.plot(x_val,y_val)\n","plt.scatter(x_val,y_val)\n","plt.title('Number of Topics vs. Coherence')\n","plt.xlabel('Number of Topics')\n","plt.ylabel('Coherence')\n","plt.xticks(x_val)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Lda = gensim.models.ldamodel.LdaModel\n","ldamodel = Lda(doc_term_matrix, num_topics=19, id2word = dictionary, passes=40,\\\n","               iterations=200,  chunksize = 10000, eval_every = None, random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ldamodel.show_topics(19, num_words=30, formatted=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pyLDAvis.gensim\n","\n","topic_data =  pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, mds = 'pcoa', R=10, sort_topics=False)\n","pyLDAvis.display(topic_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.utils import simple_preprocess\n","from nltk.corpus import stopwords\n","import pyLDAvis\n","import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","import nltk\n","\n","# NLTK-Stopwörter herunterladen\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","\n","# Datenvorverarbeitung und Tokenisierung\n","def preprocess_lyrics(lyrics):\n","    return [word for word in simple_preprocess(lyrics) if word not in stop_words]\n","\n","\n","df['tokens'] = df['lyrics'].apply(preprocess_lyrics)\n","\n","\n","# Erstelle Wörterbuch und Korpus\n","dictionary = corpora.Dictionary(df['tokens'])\n","corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n","\n","# LDA-Modellierung\n","num_topics = 23\n","lda_model = LdaModel(\n","    corpus=corpus, \n","    id2word=dictionary, \n","    num_topics=num_topics, \n","    random_state=42, \n","    update_every=1, \n","    chunksize=10,\n","    passes=20, \n","    alpha='auto', \n","    per_word_topics=True\n",")\n","\n","# Themen anzeigen\n","for idx, topic in lda_model.print_topics(num_words=15):\n","    print(f\"Topic {idx}: {topic}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualisierung\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds='mmds', R=15)\n","pyLDAvis.show(vis, local=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Topic Modeling LDA v2"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["from nltk.tokenize import RegexpTokenizer\n","\n","lyric_corpus_tokenized = []\n","tokenizer = RegexpTokenizer(r'\\w+')\n","for lyric in df.lyrics:\n","    tokenized_lyric = tokenizer.tokenize(lyric.lower())\n","    lyric_corpus_tokenized.append(tokenized_lyric)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# filter all tokens out that are less than 3 characters long\n","for s,song in enumerate(lyric_corpus_tokenized):\n","    filtered_song = []    \n","    for token in song:\n","        if len(token) > 2 and not token.isnumeric():\n","            filtered_song.append(token)\n","    lyric_corpus_tokenized[s] = filtered_song"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/niklasfischer/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","import nltk\n","\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","for s,song in enumerate(lyric_corpus_tokenized):\n","    lemmatized_tokens = []\n","    for token in song:\n","        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n","    lyric_corpus_tokenized[s] = lemmatized_tokens"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","130000\n","140000\n","150000\n","160000\n","170000\n","180000\n","190000\n","200000\n","210000\n","220000\n","230000\n","240000\n","250000\n","260000\n","270000\n","280000\n","290000\n","300000\n","310000\n","320000\n","330000\n","340000\n","350000\n","360000\n","370000\n","380000\n","390000\n","400000\n","410000\n","420000\n","430000\n","440000\n","450000\n","460000\n","470000\n","480000\n","490000\n","500000\n","510000\n","520000\n","530000\n","540000\n","550000\n","560000\n","570000\n","580000\n","590000\n","600000\n","610000\n","620000\n","630000\n","640000\n","650000\n","660000\n","670000\n","680000\n","690000\n","700000\n","710000\n","720000\n","730000\n","740000\n","750000\n","760000\n","770000\n","780000\n","790000\n","800000\n","810000\n","820000\n","830000\n","840000\n","850000\n","860000\n","870000\n","880000\n","890000\n","900000\n","910000\n","920000\n","930000\n","940000\n","950000\n"]}],"source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","new_stop_words = [\"ooh\",\"yeah\",\"hey\",\"whoa\",\"woah\", \"ohh\", \"was\", \"mmm\", \"oooh\",\"yah\",\"yeh\",\"mmm\", \"hmm\",\"deh\",\"doh\",\"jah\",\"wa\"]\n","stop_words.extend(new_stop_words)\n","for s,song in enumerate(lyric_corpus_tokenized):\n","    if s % 10_000 == 0:\n","        print(s)\n","    filtered_text = []    \n","    for token in song:\n","        if token not in stop_words:\n","            filtered_text.append(token)\n","    lyric_corpus_tokenized[s] = filtered_text"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from gensim.corpora import Dictionary\n","dictionary = Dictionary(lyric_corpus_tokenized)\n","dictionary.filter_extremes(no_below = 100, no_above = 0.8)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["from gensim.corpora import MmCorpus\n","gensim_corpus = [dictionary.doc2bow(song) for song in lyric_corpus_tokenized]\n","temp = dictionary[0]\n","id2word = dictionary.id2token"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["chunksize = 2000\n","passes = 20\n","iterations = 400\n","num_topics = 19 # k-value"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["from gensim.models import LdaModel\n","lda_model = LdaModel(\n","    corpus=gensim_corpus,\n","    id2word=id2word,\n","    chunksize=chunksize,\n","    alpha='auto',\n","    eta='auto',\n","    iterations=iterations,\n","    num_topics=num_topics,\n","    passes=passes\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4207146714878665\n"]}],"source":["from gensim.models.coherencemodel import CoherenceModel\n","coherencemodel = CoherenceModel(model=lda_model, texts=lyric_corpus_tokenized, dictionary=dictionary, coherence='c_v')\n","print(coherencemodel.get_coherence())"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","vis_data = gensimvis.prepare(lda_model, gensim_corpus, dictionary)\n","#pyLDAvis.display(vis_data)\n","pyLDAvis.save_html(vis_data, f'./Lyrics_LDA_k_{str(num_topics)}_n_{str(len(df))}.html')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2805070,"sourceId":4840139,"sourceType":"datasetVersion"}],"dockerImageVersionId":30407,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
